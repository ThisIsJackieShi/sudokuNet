{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sudoku.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Link with your google drive"
      ],
      "metadata": {
        "id": "m79eFV9DdNTx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toa8bOcSbAoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e69520e-b74c-4f07-fbff-613567390b17"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n",
        "!ls /gdrive"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive/; to attempt to forcibly remount, call drive.mount(\"/gdrive/\", force_remount=True).\n",
            "MyDrive  Shareddrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset\n",
        "You need kaggle.json from https://www.kaggle.com/bryanpark/sudoku"
      ],
      "metadata": {
        "id": "PyEwhLc5dQJK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3dYUF5zbn2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e3cf59-60f8-4c80-a9d7-e2e8b23f9935"
      },
      "source": [
        "import os\n",
        "! pip install -q kaggle\n",
        "\n",
        "if not os.path.exists('/content/sudoku.csv'):\n",
        "  if not os.path.exists('~/.kaggle'):\n",
        "    ! mkdir ~/.kaggle\n",
        "  ! cp /gdrive/MyDrive/kaggle.json ~/.kaggle/\n",
        "  ! chmod 600 ~/.kaggle/kaggle.json\n",
        "  ! kaggle datasets download -d bryanpark/sudoku\n",
        "  ! unzip -a sudoku.zip\n",
        "\n",
        "if not os.path.exists('/content/pt_util.py'):\n",
        " ! cp /gdrive/MyDrive/pt_util.py /content"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/gdrive/MyDrive/pt_util.py': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages needed"
      ],
      "metadata": {
        "id": "W-aCBZZqdazO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOk7_-5HgP3I"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import h5py\n",
        "import sys\n",
        "import pandas as pd\n",
        "#import pt_util"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Data\n",
        "Process the data from csv into numpy"
      ],
      "metadata": {
        "id": "W94bPR5Idgho"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6fjc0wgU1j"
      },
      "source": [
        "quizzes = np.zeros((1000000, 81), np.int32)\n",
        "solutions = np.zeros((1000000, 81), np.int32)\n",
        "data = np.zeros((1000000, 2, 81), np.int32)\n",
        "for i, line in enumerate(open('sudoku.csv', 'r').read().splitlines()[1:]):\n",
        "    quiz, solution = line.split(\",\")\n",
        "    for j, q_s in enumerate(zip(quiz, solution)):\n",
        "        q, s = q_s\n",
        "        quizzes[i, j] = q\n",
        "        solutions[i, j] = s\n",
        "        data[i, 0, j] = q\n",
        "        data[i, 1, j] = s\n",
        "quizzes = quizzes.reshape((-1, 9, 9))\n",
        "solutions = solutions.reshape((-1, 9, 9))\n",
        "data = data.reshape((-1, 2, 9, 9))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reshape the data to the preference of our neural network"
      ],
      "metadata": {
        "id": "4SRobQbCdl49"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27BnPMSF7Ww2"
      },
      "source": [
        "quizzes = quizzes.reshape((-1, 1, 9, 9))\n",
        "solutions = solutions.reshape((-1, 1, 9, 9))\n",
        "data = data.reshape((-1, 2, 1, 9, 9))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing data into test and train and examine the data"
      ],
      "metadata": {
        "id": "DKHod4srdtHI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhMyKmHDlhJG",
        "outputId": "d6b42a6f-599e-47bc-e7fd-7fa6dab55508"
      },
      "source": [
        "train_data = data[:200000,:,:,:]\n",
        "test_data = data[200000:210000,:,:,:]\n",
        "print(train_quizzes.shape)\n",
        "print(test_solutions.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(800000, 1, 9, 9)\n",
            "(200000, 1, 9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqLKQQ1BPFpl",
        "outputId": "574f1108-e096-482b-b37e-b7e3c3bfb0ea"
      },
      "source": [
        "print(train_data[0,0,:,:])\n",
        "print(train_data[0,1,:,:])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 0 4 3 0 0 2 0 9]\n",
            "  [0 0 5 0 0 9 0 0 1]\n",
            "  [0 7 0 0 6 0 0 4 3]\n",
            "  [0 0 6 0 0 2 0 8 7]\n",
            "  [1 9 0 0 0 7 4 0 0]\n",
            "  [0 5 0 0 8 3 0 0 0]\n",
            "  [6 0 0 0 0 0 1 0 5]\n",
            "  [0 0 3 5 0 8 6 9 0]\n",
            "  [0 4 2 9 1 0 3 0 0]]]\n",
            "[[[8 6 4 3 7 1 2 5 9]\n",
            "  [3 2 5 8 4 9 7 6 1]\n",
            "  [9 7 1 2 6 5 8 4 3]\n",
            "  [4 3 6 1 9 2 5 8 7]\n",
            "  [1 9 8 6 5 7 4 3 2]\n",
            "  [2 5 7 4 8 3 9 1 6]\n",
            "  [6 8 9 7 3 4 1 2 5]\n",
            "  [7 1 3 5 2 8 6 9 4]\n",
            "  [5 4 2 9 1 6 3 7 8]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the SudokuNet\n",
        "Define customized dataloaders and use them to load data"
      ],
      "metadata": {
        "id": "L02LXxxpePVY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXYY8Fj90ei"
      },
      "source": [
        "# Data loader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class sudokuDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed = np.zeros((9, 9, 9))\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed[label[0, i, j] - 1, i, j] = 1 \n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        \n",
        "        return (torch.from_numpy(feat).float(), torch.from_numpy(processed).float())\n",
        "\n",
        "class sudokuDatasetChannel(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "      self.transform = transform\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed_label = np.zeros((9, 9, 9))\n",
        "        processed_feat = np.zeros((9, 9, 9))\n",
        "\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed_label[label[0, i, j] - 1, i, j] = 1 \n",
        "            if feat[0, i, j]:\n",
        "              processed_feat[feat[0, i, j] - 1, i, j] = 1\n",
        "        \n",
        "        return (torch.from_numpy(processed_feat).float(), torch.from_numpy(processed_label).float())\n",
        "\n",
        "class sudokuDatasetChannelFlattenLabel(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "      self.transform = transform\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed_label = np.zeros((9, 9, 9))\n",
        "        processed_feat = np.zeros((9, 9, 9))\n",
        "\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed_label[label[0, i, j] - 1, i, j] = 1 \n",
        "            if feat[0, i, j]:\n",
        "              processed_feat[feat[0, i, j] - 1, i, j] = 1\n",
        "        \n",
        "        return (torch.from_numpy(processed_feat).float(), torch.flatten(torch.from_numpy(processed_label).float()))\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qRHyBTnN2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc882ce-b657-433d-e343-5a9d7937a479"
      },
      "source": [
        "train_loader = DataLoader(sudokuDataset(train_data), batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(sudokuDataset(test_data), batch_size=1, shuffle=True)\n",
        "#print(train_loader.dataset.__getitem__(0))\n",
        "\n",
        "train_loader_channel = DataLoader(sudokuDatasetChannel(train_data), batch_size=16, shuffle=True)\n",
        "test_loader_channel = DataLoader(sudokuDatasetChannel(test_data), batch_size=1, shuffle=True)\n",
        "print(train_loader_channel.dataset.__getitem__(0))\n",
        "\n",
        "train_loader_flattern = DataLoader(sudokuDatasetChannelFlattenLabel(train_data), batch_size=16, shuffle=True)\n",
        "test_loader_flattern = DataLoader(sudokuDatasetChannelFlattenLabel(test_data), batch_size=1, shuffle=True)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.]]]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define our SudokuNet and train + test functions"
      ],
      "metadata": {
        "id": "PLBa0GiSehkz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADe3SpNBjUwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b9372a-ddd1-48f5-cd69-b8e0d95d6029"
      },
      "source": [
        "! mkdir /gdrive/MyDrive/490g1/checkpoints\n",
        "file_path = \"checkpoints\"\n",
        "class sudokuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sudokuNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 9, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(9, 1, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9*9*9, 9*9*9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "class sudokuNetChannel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sudokuNetChannel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(9, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(16, 9, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9*9*9, 9*9*9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = x.reshape((-1, 9, 9 ,9))\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "class sudokuFeatureNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sudokuNetChannel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(9, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(16, 9, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9*9*9, 9*9*9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/gdrive/MyDrive/490g1/checkpoints’: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0, verbose=1):\n",
        "  net.to(device)\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "  for epoch in range(epochs):\n",
        "    sum_loss = 0.0\n",
        "    for i, batch in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        losses.append(loss.item())\n",
        "        sum_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            if verbose:\n",
        "              print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, sum_loss / 100))\n",
        "            sum_loss = 0.0\n",
        "  return losses\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    blank_cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, label) in enumerate(test_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        output = model(data)\n",
        "        \n",
        "        '''\n",
        "        blank_mask = torch.zeros([output.shape[0], 1, 9, 9]).floats()\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              is_blank = True\n",
        "              for n in range(9):\n",
        "                if data[k, n, i, j] != 0:\n",
        "                  is_blank = False\n",
        "                  break;\n",
        "              if is_blank:\n",
        "                blank_mask[k, 0, i, k] = 1.0\n",
        "                blank_cnt += 1\n",
        "        '''\n",
        "        \n",
        "        x = torch.zeros([output.shape[0], 1, 9, 9])\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              x[k, 0, i, j] = torch.argmax(output[k, :, i, j])+1\n",
        "        \n",
        "        y = torch.zeros([label.shape[0], 1, 9, 9])\n",
        "        for k in range(label.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              y[k, 0, i, j] = torch.argmax(label[k, :, i, j])+1\n",
        "\n",
        "        correct_mask = x.eq(y.view_as(x))\n",
        "\n",
        "        num_correct = (correct_mask).sum().item()\n",
        "\n",
        "        correct += num_correct\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) /81\n",
        "    #test_accuracy = 100. * correct / blank_cnt\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset) * 81, test_accuracy))\n",
        "    return test_loss, test_accuracy"
      ],
      "metadata": {
        "id": "sQxHDaD2gl4l"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the network and printout test result"
      ],
      "metadata": {
        "id": "gB0Ud29feqAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now the actual training code\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "#torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "model = sudokuNetChannel().to(device)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY, momentum=0.9)\n",
        "train_loss = []\n",
        "train_loss += train(model, train_loader_channel, lr=0.1)\n",
        "train_loss += train(model, train_loader_channel, lr=0.05)\n",
        "train_loss += train(model, train_loader_channel, lr=0.05)\n",
        "train_loss += train(model, train_loader_channel, lr=0.01, momentum=0.5)\n",
        "train_loss += train(model, train_loader_channel, lr=0.01, momentum=0.5)\n",
        "\n",
        "# torch.save(model.state_dict(), \"/gdrive/MyDrive/490g1/checkpoints/sudokuNetChannel-1.pt\")\n",
        "# model.load_state_dict(torch.load(\"/gdrive/MyDrive/490g1/checkpoints/sudokuNetChannel-1.pt\"))\n",
        "\n",
        "test(model, test_loader_channel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVKOVcyoa3jO",
        "outputId": "27c5224b-4a44-40df-e60f-3b0a62f746aa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "[1,   100] loss: 2.19727\n",
            "[1,   200] loss: 2.19720\n",
            "[1,   300] loss: 2.19715\n",
            "[1,   400] loss: 2.19713\n",
            "[1,   500] loss: 2.19706\n",
            "[1,   600] loss: 2.19701\n",
            "[1,   700] loss: 2.19690\n",
            "[1,   800] loss: 2.19674\n",
            "[1,   900] loss: 2.19633\n",
            "[1,  1000] loss: 2.19527\n",
            "[1,  1100] loss: 2.18920\n",
            "[1,  1200] loss: 2.15945\n",
            "[1,  1300] loss: 2.09946\n",
            "[1,  1400] loss: 2.00262\n",
            "[1,  1500] loss: 1.89289\n",
            "[1,  1600] loss: 1.77924\n",
            "[1,  1700] loss: 1.67488\n",
            "[1,  1800] loss: 1.57529\n",
            "[1,  1900] loss: 1.47866\n",
            "[1,  2000] loss: 1.39845\n",
            "[1,  2100] loss: 1.31629\n",
            "[1,  2200] loss: 1.25926\n",
            "[1,  2300] loss: 1.20423\n",
            "[1,  2400] loss: 1.16153\n",
            "[1,  2500] loss: 1.13123\n",
            "[1,  2600] loss: 1.09758\n",
            "[1,  2700] loss: 1.07021\n",
            "[1,  2800] loss: 1.04382\n",
            "[1,  2900] loss: 1.02353\n",
            "[1,  3000] loss: 0.99457\n",
            "[1,  3100] loss: 0.97741\n",
            "[1,  3200] loss: 0.95887\n",
            "[1,  3300] loss: 0.93947\n",
            "[1,  3400] loss: 0.91497\n",
            "[1,  3500] loss: 0.90990\n",
            "[1,  3600] loss: 0.89718\n",
            "[1,  3700] loss: 0.87590\n",
            "[1,  3800] loss: 0.86229\n",
            "[1,  3900] loss: 0.85543\n",
            "[1,  4000] loss: 0.84662\n",
            "[1,  4100] loss: 0.82686\n",
            "[1,  4200] loss: 0.82382\n",
            "[1,  4300] loss: 0.80815\n",
            "[1,  4400] loss: 0.80371\n",
            "[1,  4500] loss: 0.79122\n",
            "[1,  4600] loss: 0.78014\n",
            "[1,  4700] loss: 0.76767\n",
            "[1,  4800] loss: 0.75908\n",
            "[1,  4900] loss: 0.74773\n",
            "[1,  5000] loss: 0.73825\n",
            "[1,  5100] loss: 0.73326\n",
            "[1,  5200] loss: 0.72376\n",
            "[1,  5300] loss: 0.72526\n",
            "[1,  5400] loss: 0.71412\n",
            "[1,  5500] loss: 0.71025\n",
            "[1,  5600] loss: 0.69846\n",
            "[1,  5700] loss: 0.70053\n",
            "[1,  5800] loss: 0.69752\n",
            "[1,  5900] loss: 0.69586\n",
            "[1,  6000] loss: 0.67949\n",
            "[1,  6100] loss: 0.67305\n",
            "[1,  6200] loss: 0.67672\n",
            "[1,  6300] loss: 0.66397\n",
            "[1,  6400] loss: 0.66180\n",
            "[1,  6500] loss: 0.66071\n",
            "[1,  6600] loss: 0.65618\n",
            "[1,  6700] loss: 0.65172\n",
            "[1,  6800] loss: 0.64513\n",
            "[1,  6900] loss: 0.64407\n",
            "[1,  7000] loss: 0.63571\n",
            "[1,  7100] loss: 0.63353\n",
            "[1,  7200] loss: 0.62991\n",
            "[1,  7300] loss: 0.62565\n",
            "[1,  7400] loss: 0.62653\n",
            "[1,  7500] loss: 0.62186\n",
            "[1,  7600] loss: 0.61314\n",
            "[1,  7700] loss: 0.60801\n",
            "[1,  7800] loss: 0.61276\n",
            "[1,  7900] loss: 0.60434\n",
            "[1,  8000] loss: 0.60465\n",
            "[1,  8100] loss: 0.59870\n",
            "[1,  8200] loss: 0.60024\n",
            "[1,  8300] loss: 0.59580\n",
            "[1,  8400] loss: 0.58640\n",
            "[1,  8500] loss: 0.59070\n",
            "[1,  8600] loss: 0.58332\n",
            "[1,  8700] loss: 0.57875\n",
            "[1,  8800] loss: 0.58098\n",
            "[1,  8900] loss: 0.57303\n",
            "[1,  9000] loss: 0.56784\n",
            "[1,  9100] loss: 0.56840\n",
            "[1,  9200] loss: 0.56491\n",
            "[1,  9300] loss: 0.56719\n",
            "[1,  9400] loss: 0.56417\n",
            "[1,  9500] loss: 0.57377\n",
            "[1,  9600] loss: 0.56581\n",
            "[1,  9700] loss: 0.55687\n",
            "[1,  9800] loss: 0.56055\n",
            "[1,  9900] loss: 0.56194\n",
            "[1, 10000] loss: 0.55883\n",
            "[1, 10100] loss: 0.54550\n",
            "[1, 10200] loss: 0.55638\n",
            "[1, 10300] loss: 0.55246\n",
            "[1, 10400] loss: 0.55904\n",
            "[1, 10500] loss: 0.55323\n",
            "[1, 10600] loss: 0.54739\n",
            "[1, 10700] loss: 0.54571\n",
            "[1, 10800] loss: 0.54815\n",
            "[1, 10900] loss: 0.54853\n",
            "[1, 11000] loss: 0.54097\n",
            "[1, 11100] loss: 0.54504\n",
            "[1, 11200] loss: 0.53929\n",
            "[1, 11300] loss: 0.53559\n",
            "[1, 11400] loss: 0.54535\n",
            "[1, 11500] loss: 0.53746\n",
            "[1, 11600] loss: 0.53316\n",
            "[1, 11700] loss: 0.53649\n",
            "[1, 11800] loss: 0.53107\n",
            "[1, 11900] loss: 0.53060\n",
            "[1, 12000] loss: 0.53821\n",
            "[1, 12100] loss: 0.52932\n",
            "[1, 12200] loss: 0.52979\n",
            "[1, 12300] loss: 0.52588\n",
            "[1, 12400] loss: 0.52793\n",
            "[1, 12500] loss: 0.52409\n",
            "[1,   100] loss: 0.45707\n",
            "[1,   200] loss: 0.45042\n",
            "[1,   300] loss: 0.44446\n",
            "[1,   400] loss: 0.44058\n",
            "[1,   500] loss: 0.44065\n",
            "[1,   600] loss: 0.43535\n",
            "[1,   700] loss: 0.43512\n",
            "[1,   800] loss: 0.43593\n",
            "[1,   900] loss: 0.42693\n",
            "[1,  1000] loss: 0.43288\n",
            "[1,  1100] loss: 0.42843\n",
            "[1,  1200] loss: 0.43194\n",
            "[1,  1300] loss: 0.43159\n",
            "[1,  1400] loss: 0.43133\n",
            "[1,  1500] loss: 0.42793\n",
            "[1,  1600] loss: 0.42936\n",
            "[1,  1700] loss: 0.42793\n",
            "[1,  1800] loss: 0.42592\n",
            "[1,  1900] loss: 0.42800\n",
            "[1,  2000] loss: 0.42187\n",
            "[1,  2100] loss: 0.42288\n",
            "[1,  2200] loss: 0.42371\n",
            "[1,  2300] loss: 0.42651\n",
            "[1,  2400] loss: 0.42295\n",
            "[1,  2500] loss: 0.42212\n",
            "[1,  2600] loss: 0.42005\n",
            "[1,  2700] loss: 0.41843\n",
            "[1,  2800] loss: 0.42641\n",
            "[1,  2900] loss: 0.42329\n",
            "[1,  3000] loss: 0.42225\n",
            "[1,  3100] loss: 0.42053\n",
            "[1,  3200] loss: 0.41952\n",
            "[1,  3300] loss: 0.41925\n",
            "[1,  3400] loss: 0.42361\n",
            "[1,  3500] loss: 0.42025\n",
            "[1,  3600] loss: 0.41791\n",
            "[1,  3700] loss: 0.41632\n",
            "[1,  3800] loss: 0.41578\n",
            "[1,  3900] loss: 0.41918\n",
            "[1,  4000] loss: 0.41767\n",
            "[1,  4100] loss: 0.42029\n",
            "[1,  4200] loss: 0.41621\n",
            "[1,  4300] loss: 0.41674\n",
            "[1,  4400] loss: 0.41799\n",
            "[1,  4500] loss: 0.41527\n",
            "[1,  4600] loss: 0.41548\n",
            "[1,  4700] loss: 0.41554\n",
            "[1,  4800] loss: 0.41952\n",
            "[1,  4900] loss: 0.41219\n",
            "[1,  5000] loss: 0.41722\n",
            "[1,  5100] loss: 0.42006\n",
            "[1,  5200] loss: 0.41789\n",
            "[1,  5300] loss: 0.41572\n",
            "[1,  5400] loss: 0.41743\n",
            "[1,  5500] loss: 0.41943\n",
            "[1,  5600] loss: 0.41295\n",
            "[1,  5700] loss: 0.41757\n",
            "[1,  5800] loss: 0.41573\n",
            "[1,  5900] loss: 0.41239\n",
            "[1,  6000] loss: 0.41231\n",
            "[1,  6100] loss: 0.41513\n",
            "[1,  6200] loss: 0.41409\n",
            "[1,  6300] loss: 0.41120\n",
            "[1,  6400] loss: 0.41476\n",
            "[1,  6500] loss: 0.41358\n",
            "[1,  6600] loss: 0.41199\n",
            "[1,  6700] loss: 0.41053\n",
            "[1,  6800] loss: 0.41173\n",
            "[1,  6900] loss: 0.41239\n",
            "[1,  7000] loss: 0.41290\n",
            "[1,  7100] loss: 0.41358\n",
            "[1,  7200] loss: 0.41072\n",
            "[1,  7300] loss: 0.41439\n",
            "[1,  7400] loss: 0.41706\n",
            "[1,  7500] loss: 0.41316\n",
            "[1,  7600] loss: 0.41426\n",
            "[1,  7700] loss: 0.41583\n",
            "[1,  7800] loss: 0.40934\n",
            "[1,  7900] loss: 0.41418\n",
            "[1,  8000] loss: 0.41062\n",
            "[1,  8100] loss: 0.41057\n",
            "[1,  8200] loss: 0.41201\n",
            "[1,  8300] loss: 0.41143\n",
            "[1,  8400] loss: 0.40938\n",
            "[1,  8500] loss: 0.41429\n",
            "[1,  8600] loss: 0.40951\n",
            "[1,  8700] loss: 0.41196\n",
            "[1,  8800] loss: 0.40925\n",
            "[1,  8900] loss: 0.40916\n",
            "[1,  9000] loss: 0.41182\n",
            "[1,  9100] loss: 0.40979\n",
            "[1,  9200] loss: 0.40559\n",
            "[1,  9300] loss: 0.41243\n",
            "[1,  9400] loss: 0.40984\n",
            "[1,  9500] loss: 0.41172\n",
            "[1,  9600] loss: 0.40847\n",
            "[1,  9700] loss: 0.40655\n",
            "[1,  9800] loss: 0.41081\n",
            "[1,  9900] loss: 0.41027\n",
            "[1, 10000] loss: 0.40846\n",
            "[1, 10100] loss: 0.40721\n",
            "[1, 10200] loss: 0.40734\n",
            "[1, 10300] loss: 0.41020\n",
            "[1, 10400] loss: 0.40693\n",
            "[1, 10500] loss: 0.40599\n",
            "[1, 10600] loss: 0.40747\n",
            "[1, 10700] loss: 0.40690\n",
            "[1, 10800] loss: 0.40540\n",
            "[1, 10900] loss: 0.41064\n",
            "[1, 11000] loss: 0.40727\n",
            "[1, 11100] loss: 0.40665\n",
            "[1, 11200] loss: 0.40320\n",
            "[1, 11300] loss: 0.40515\n",
            "[1, 11400] loss: 0.40672\n",
            "[1, 11500] loss: 0.40526\n",
            "[1, 11600] loss: 0.40270\n",
            "[1, 11700] loss: 0.40868\n",
            "[1, 11800] loss: 0.40678\n",
            "[1, 11900] loss: 0.40540\n",
            "[1, 12000] loss: 0.40612\n",
            "[1, 12100] loss: 0.40563\n",
            "[1, 12200] loss: 0.40198\n",
            "[1, 12300] loss: 0.40219\n",
            "[1, 12400] loss: 0.40634\n",
            "[1, 12500] loss: 0.40614\n",
            "[1,   100] loss: 0.37928\n",
            "[1,   200] loss: 0.38306\n",
            "[1,   300] loss: 0.38843\n",
            "[1,   400] loss: 0.38417\n",
            "[1,   500] loss: 0.39030\n",
            "[1,   600] loss: 0.39078\n",
            "[1,   700] loss: 0.38542\n",
            "[1,   800] loss: 0.38790\n",
            "[1,   900] loss: 0.39088\n",
            "[1,  1000] loss: 0.38927\n",
            "[1,  1100] loss: 0.39176\n",
            "[1,  1200] loss: 0.38872\n",
            "[1,  1300] loss: 0.39430\n",
            "[1,  1400] loss: 0.39553\n",
            "[1,  1500] loss: 0.39327\n",
            "[1,  1600] loss: 0.39168\n",
            "[1,  1700] loss: 0.39522\n",
            "[1,  1800] loss: 0.39323\n",
            "[1,  1900] loss: 0.39811\n",
            "[1,  2000] loss: 0.39964\n",
            "[1,  2100] loss: 0.39347\n",
            "[1,  2200] loss: 0.39650\n",
            "[1,  2300] loss: 0.39306\n",
            "[1,  2400] loss: 0.39646\n",
            "[1,  2500] loss: 0.39672\n",
            "[1,  2600] loss: 0.39763\n",
            "[1,  2700] loss: 0.39424\n",
            "[1,  2800] loss: 0.39717\n",
            "[1,  2900] loss: 0.39546\n",
            "[1,  3000] loss: 0.39539\n",
            "[1,  3100] loss: 0.39675\n",
            "[1,  3200] loss: 0.39508\n",
            "[1,  3300] loss: 0.39444\n",
            "[1,  3400] loss: 0.39564\n",
            "[1,  3500] loss: 0.39858\n",
            "[1,  3600] loss: 0.39950\n",
            "[1,  3700] loss: 0.40069\n",
            "[1,  3800] loss: 0.39550\n",
            "[1,  3900] loss: 0.39769\n",
            "[1,  4000] loss: 0.39718\n",
            "[1,  4100] loss: 0.39585\n",
            "[1,  4200] loss: 0.39887\n",
            "[1,  4300] loss: 0.39574\n",
            "[1,  4400] loss: 0.39628\n",
            "[1,  4500] loss: 0.39806\n",
            "[1,  4600] loss: 0.39485\n",
            "[1,  4700] loss: 0.40197\n",
            "[1,  4800] loss: 0.39996\n",
            "[1,  4900] loss: 0.40224\n",
            "[1,  5000] loss: 0.39967\n",
            "[1,  5100] loss: 0.39843\n",
            "[1,  5200] loss: 0.39748\n",
            "[1,  5300] loss: 0.39990\n",
            "[1,  5400] loss: 0.40016\n",
            "[1,  5500] loss: 0.40170\n",
            "[1,  5600] loss: 0.40038\n",
            "[1,  5700] loss: 0.39885\n",
            "[1,  5800] loss: 0.39709\n",
            "[1,  5900] loss: 0.39995\n",
            "[1,  6000] loss: 0.39747\n",
            "[1,  6100] loss: 0.39474\n",
            "[1,  6200] loss: 0.39769\n",
            "[1,  6300] loss: 0.39861\n",
            "[1,  6400] loss: 0.39736\n",
            "[1,  6500] loss: 0.39958\n",
            "[1,  6600] loss: 0.39459\n",
            "[1,  6700] loss: 0.39784\n",
            "[1,  6800] loss: 0.40062\n",
            "[1,  6900] loss: 0.39839\n",
            "[1,  7000] loss: 0.39496\n",
            "[1,  7100] loss: 0.39548\n",
            "[1,  7200] loss: 0.39912\n",
            "[1,  7300] loss: 0.40329\n",
            "[1,  7400] loss: 0.39840\n",
            "[1,  7500] loss: 0.39706\n",
            "[1,  7600] loss: 0.39526\n",
            "[1,  7700] loss: 0.39653\n",
            "[1,  7800] loss: 0.39762\n",
            "[1,  7900] loss: 0.39687\n",
            "[1,  8000] loss: 0.39815\n",
            "[1,  8100] loss: 0.39537\n",
            "[1,  8200] loss: 0.39513\n",
            "[1,  8300] loss: 0.39986\n",
            "[1,  8400] loss: 0.39563\n",
            "[1,  8500] loss: 0.39639\n",
            "[1,  8600] loss: 0.39417\n",
            "[1,  8700] loss: 0.39841\n",
            "[1,  8800] loss: 0.39997\n",
            "[1,  8900] loss: 0.39766\n",
            "[1,  9000] loss: 0.39560\n",
            "[1,  9100] loss: 0.39357\n",
            "[1,  9200] loss: 0.39544\n",
            "[1,  9300] loss: 0.39526\n",
            "[1,  9400] loss: 0.39879\n",
            "[1,  9500] loss: 0.39608\n",
            "[1,  9600] loss: 0.39374\n",
            "[1,  9700] loss: 0.39586\n",
            "[1,  9800] loss: 0.39533\n",
            "[1,  9900] loss: 0.39324\n",
            "[1, 10000] loss: 0.39760\n",
            "[1, 10100] loss: 0.39515\n",
            "[1, 10200] loss: 0.39484\n",
            "[1, 10300] loss: 0.39625\n",
            "[1, 10400] loss: 0.39792\n",
            "[1, 10500] loss: 0.39522\n",
            "[1, 10600] loss: 0.39341\n",
            "[1, 10700] loss: 0.39721\n",
            "[1, 10800] loss: 0.39509\n",
            "[1, 10900] loss: 0.39768\n",
            "[1, 11000] loss: 0.39666\n",
            "[1, 11100] loss: 0.39372\n",
            "[1, 11200] loss: 0.39732\n",
            "[1, 11300] loss: 0.39696\n",
            "[1, 11400] loss: 0.39415\n",
            "[1, 11500] loss: 0.39308\n",
            "[1, 11600] loss: 0.39553\n",
            "[1, 11700] loss: 0.39613\n",
            "[1, 11800] loss: 0.39548\n",
            "[1, 11900] loss: 0.39431\n",
            "[1, 12000] loss: 0.39340\n",
            "[1, 12100] loss: 0.39265\n",
            "[1, 12200] loss: 0.39486\n",
            "[1, 12300] loss: 0.39154\n",
            "[1, 12400] loss: 0.39812\n",
            "[1, 12500] loss: 0.39276\n",
            "[1,   100] loss: 0.35088\n",
            "[1,   200] loss: 0.33862\n",
            "[1,   300] loss: 0.33566\n",
            "[1,   400] loss: 0.33291\n",
            "[1,   500] loss: 0.32864\n",
            "[1,   600] loss: 0.33079\n",
            "[1,   700] loss: 0.32857\n",
            "[1,   800] loss: 0.32734\n",
            "[1,   900] loss: 0.32709\n",
            "[1,  1000] loss: 0.32518\n",
            "[1,  1100] loss: 0.32350\n",
            "[1,  1200] loss: 0.32560\n",
            "[1,  1300] loss: 0.32176\n",
            "[1,  1400] loss: 0.32471\n",
            "[1,  1500] loss: 0.32392\n",
            "[1,  1600] loss: 0.32453\n",
            "[1,  1700] loss: 0.32331\n",
            "[1,  1800] loss: 0.32693\n",
            "[1,  1900] loss: 0.32300\n",
            "[1,  2000] loss: 0.32468\n",
            "[1,  2100] loss: 0.31990\n",
            "[1,  2200] loss: 0.32068\n",
            "[1,  2300] loss: 0.32200\n",
            "[1,  2400] loss: 0.32006\n",
            "[1,  2500] loss: 0.32178\n",
            "[1,  2600] loss: 0.32054\n",
            "[1,  2700] loss: 0.32120\n",
            "[1,  2800] loss: 0.31771\n",
            "[1,  2900] loss: 0.32076\n",
            "[1,  3000] loss: 0.32270\n",
            "[1,  3100] loss: 0.32282\n",
            "[1,  3200] loss: 0.31819\n",
            "[1,  3300] loss: 0.32274\n",
            "[1,  3400] loss: 0.31965\n",
            "[1,  3500] loss: 0.32014\n",
            "[1,  3600] loss: 0.32129\n",
            "[1,  3700] loss: 0.32164\n",
            "[1,  3800] loss: 0.32167\n",
            "[1,  3900] loss: 0.32009\n",
            "[1,  4000] loss: 0.32143\n",
            "[1,  4100] loss: 0.32228\n",
            "[1,  4200] loss: 0.32147\n",
            "[1,  4300] loss: 0.31784\n",
            "[1,  4400] loss: 0.31939\n",
            "[1,  4500] loss: 0.31704\n",
            "[1,  4600] loss: 0.32021\n",
            "[1,  4700] loss: 0.31843\n",
            "[1,  4800] loss: 0.31870\n",
            "[1,  4900] loss: 0.31786\n",
            "[1,  5000] loss: 0.31776\n",
            "[1,  5100] loss: 0.31823\n",
            "[1,  5200] loss: 0.32078\n",
            "[1,  5300] loss: 0.32173\n",
            "[1,  5400] loss: 0.32165\n",
            "[1,  5500] loss: 0.32043\n",
            "[1,  5600] loss: 0.31789\n",
            "[1,  5700] loss: 0.32156\n",
            "[1,  5800] loss: 0.31900\n",
            "[1,  5900] loss: 0.31551\n",
            "[1,  6000] loss: 0.31874\n",
            "[1,  6100] loss: 0.31935\n",
            "[1,  6200] loss: 0.31662\n",
            "[1,  6300] loss: 0.31824\n",
            "[1,  6400] loss: 0.32005\n",
            "[1,  6500] loss: 0.32063\n",
            "[1,  6600] loss: 0.31788\n",
            "[1,  6700] loss: 0.31790\n",
            "[1,  6800] loss: 0.31947\n",
            "[1,  6900] loss: 0.31617\n",
            "[1,  7000] loss: 0.31641\n",
            "[1,  7100] loss: 0.32033\n",
            "[1,  7200] loss: 0.31490\n",
            "[1,  7300] loss: 0.31687\n",
            "[1,  7400] loss: 0.31837\n",
            "[1,  7500] loss: 0.31875\n",
            "[1,  7600] loss: 0.31489\n",
            "[1,  7700] loss: 0.31447\n",
            "[1,  7800] loss: 0.31874\n",
            "[1,  7900] loss: 0.31803\n",
            "[1,  8000] loss: 0.31486\n",
            "[1,  8100] loss: 0.32029\n",
            "[1,  8200] loss: 0.31527\n",
            "[1,  8300] loss: 0.31649\n",
            "[1,  8400] loss: 0.32157\n",
            "[1,  8500] loss: 0.31979\n",
            "[1,  8600] loss: 0.31435\n",
            "[1,  8700] loss: 0.31478\n",
            "[1,  8800] loss: 0.31557\n",
            "[1,  8900] loss: 0.31662\n",
            "[1,  9000] loss: 0.31556\n",
            "[1,  9100] loss: 0.31497\n",
            "[1,  9200] loss: 0.31680\n",
            "[1,  9300] loss: 0.31511\n",
            "[1,  9400] loss: 0.31572\n",
            "[1,  9500] loss: 0.31656\n",
            "[1,  9600] loss: 0.31502\n",
            "[1,  9700] loss: 0.31361\n",
            "[1,  9800] loss: 0.31663\n",
            "[1,  9900] loss: 0.31555\n",
            "[1, 10000] loss: 0.31738\n",
            "[1, 10100] loss: 0.31482\n",
            "[1, 10200] loss: 0.31543\n",
            "[1, 10300] loss: 0.31262\n",
            "[1, 10400] loss: 0.31470\n",
            "[1, 10500] loss: 0.31464\n",
            "[1, 10600] loss: 0.31459\n",
            "[1, 10700] loss: 0.31375\n",
            "[1, 10800] loss: 0.31354\n",
            "[1, 10900] loss: 0.31374\n",
            "[1, 11000] loss: 0.31498\n",
            "[1, 11100] loss: 0.31449\n",
            "[1, 11200] loss: 0.31836\n",
            "[1, 11300] loss: 0.31414\n",
            "[1, 11400] loss: 0.31549\n",
            "[1, 11500] loss: 0.31920\n",
            "[1, 11600] loss: 0.31222\n",
            "[1, 11700] loss: 0.31568\n",
            "[1, 11800] loss: 0.31458\n",
            "[1, 11900] loss: 0.31450\n",
            "[1, 12000] loss: 0.31504\n",
            "[1, 12100] loss: 0.31614\n",
            "[1, 12200] loss: 0.31589\n",
            "[1, 12300] loss: 0.31477\n",
            "[1, 12400] loss: 0.31765\n",
            "[1, 12500] loss: 0.31364\n",
            "[1,   100] loss: 0.30939\n",
            "[1,   200] loss: 0.30903\n",
            "[1,   300] loss: 0.30707\n",
            "[1,   400] loss: 0.30618\n",
            "[1,   500] loss: 0.30859\n",
            "[1,   600] loss: 0.30891\n",
            "[1,   700] loss: 0.30818\n",
            "[1,   800] loss: 0.30923\n",
            "[1,   900] loss: 0.30902\n",
            "[1,  1000] loss: 0.30759\n",
            "[1,  1100] loss: 0.30940\n",
            "[1,  1200] loss: 0.30746\n",
            "[1,  1300] loss: 0.31227\n",
            "[1,  1400] loss: 0.31163\n",
            "[1,  1500] loss: 0.30814\n",
            "[1,  1600] loss: 0.31015\n",
            "[1,  1700] loss: 0.30843\n",
            "[1,  1800] loss: 0.30862\n",
            "[1,  1900] loss: 0.30909\n",
            "[1,  2000] loss: 0.30974\n",
            "[1,  2100] loss: 0.30903\n",
            "[1,  2200] loss: 0.30732\n",
            "[1,  2300] loss: 0.30782\n",
            "[1,  2400] loss: 0.30864\n",
            "[1,  2500] loss: 0.30762\n",
            "[1,  2600] loss: 0.30942\n",
            "[1,  2700] loss: 0.30704\n",
            "[1,  2800] loss: 0.30824\n",
            "[1,  2900] loss: 0.30667\n",
            "[1,  3000] loss: 0.30935\n",
            "[1,  3100] loss: 0.30586\n",
            "[1,  3200] loss: 0.30996\n",
            "[1,  3300] loss: 0.30747\n",
            "[1,  3400] loss: 0.30975\n",
            "[1,  3500] loss: 0.30702\n",
            "[1,  3600] loss: 0.30829\n",
            "[1,  3700] loss: 0.31057\n",
            "[1,  3800] loss: 0.30725\n",
            "[1,  3900] loss: 0.30999\n",
            "[1,  4000] loss: 0.30939\n",
            "[1,  4100] loss: 0.30896\n",
            "[1,  4200] loss: 0.30656\n",
            "[1,  4300] loss: 0.30884\n",
            "[1,  4400] loss: 0.30947\n",
            "[1,  4500] loss: 0.30469\n",
            "[1,  4600] loss: 0.30906\n",
            "[1,  4700] loss: 0.30626\n",
            "[1,  4800] loss: 0.30752\n",
            "[1,  4900] loss: 0.30757\n",
            "[1,  5000] loss: 0.30946\n",
            "[1,  5100] loss: 0.30874\n",
            "[1,  5200] loss: 0.30721\n",
            "[1,  5300] loss: 0.30344\n",
            "[1,  5400] loss: 0.30770\n",
            "[1,  5500] loss: 0.30877\n",
            "[1,  5600] loss: 0.30831\n",
            "[1,  5700] loss: 0.30451\n",
            "[1,  5800] loss: 0.30633\n",
            "[1,  5900] loss: 0.30934\n",
            "[1,  6000] loss: 0.30775\n",
            "[1,  6100] loss: 0.30726\n",
            "[1,  6200] loss: 0.30622\n",
            "[1,  6300] loss: 0.30888\n",
            "[1,  6400] loss: 0.30795\n",
            "[1,  6500] loss: 0.30504\n",
            "[1,  6600] loss: 0.30805\n",
            "[1,  6700] loss: 0.30817\n",
            "[1,  6800] loss: 0.30630\n",
            "[1,  6900] loss: 0.30844\n",
            "[1,  7000] loss: 0.30795\n",
            "[1,  7100] loss: 0.30543\n",
            "[1,  7200] loss: 0.30873\n",
            "[1,  7300] loss: 0.30657\n",
            "[1,  7400] loss: 0.30716\n",
            "[1,  7500] loss: 0.30571\n",
            "[1,  7600] loss: 0.30438\n",
            "[1,  7700] loss: 0.30557\n",
            "[1,  7800] loss: 0.31206\n",
            "[1,  7900] loss: 0.30556\n",
            "[1,  8000] loss: 0.30680\n",
            "[1,  8100] loss: 0.30893\n",
            "[1,  8200] loss: 0.30722\n",
            "[1,  8300] loss: 0.30541\n",
            "[1,  8400] loss: 0.30507\n",
            "[1,  8500] loss: 0.30769\n",
            "[1,  8600] loss: 0.30682\n",
            "[1,  8700] loss: 0.30761\n",
            "[1,  8800] loss: 0.30437\n",
            "[1,  8900] loss: 0.30531\n",
            "[1,  9000] loss: 0.30772\n",
            "[1,  9100] loss: 0.30632\n",
            "[1,  9200] loss: 0.30826\n",
            "[1,  9300] loss: 0.30630\n",
            "[1,  9400] loss: 0.30576\n",
            "[1,  9500] loss: 0.30596\n",
            "[1,  9600] loss: 0.30663\n",
            "[1,  9700] loss: 0.30599\n",
            "[1,  9800] loss: 0.30511\n",
            "[1,  9900] loss: 0.30719\n",
            "[1, 10000] loss: 0.30239\n",
            "[1, 10100] loss: 0.30754\n",
            "[1, 10200] loss: 0.30730\n",
            "[1, 10300] loss: 0.31011\n",
            "[1, 10400] loss: 0.30720\n",
            "[1, 10500] loss: 0.30695\n",
            "[1, 10600] loss: 0.30694\n",
            "[1, 10700] loss: 0.30477\n",
            "[1, 10800] loss: 0.30710\n",
            "[1, 10900] loss: 0.30762\n",
            "[1, 11000] loss: 0.30554\n",
            "[1, 11100] loss: 0.30600\n",
            "[1, 11200] loss: 0.30759\n",
            "[1, 11300] loss: 0.30457\n",
            "[1, 11400] loss: 0.30546\n",
            "[1, 11500] loss: 0.30563\n",
            "[1, 11600] loss: 0.30726\n",
            "[1, 11700] loss: 0.30325\n",
            "[1, 11800] loss: 0.30724\n",
            "[1, 11900] loss: 0.30596\n",
            "[1, 12000] loss: 0.30580\n",
            "[1, 12100] loss: 0.30438\n",
            "[1, 12200] loss: 0.30929\n",
            "[1, 12300] loss: 0.30464\n",
            "[1, 12400] loss: 0.31011\n",
            "[1, 12500] loss: 0.30606\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 690107/810000 (85%)\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 85.19839506172839)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bW2vKUaYyf",
        "outputId": "ae14aab4-be27-4656-e696-132bb7fdb551"
      },
      "source": [
        "test_losses = test(model, test_loader_channel)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 690107/810000 (85%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the loss graph"
      ],
      "metadata": {
        "id": "wGGnHpYde_gk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "dw5Yn_p7ItE-",
        "outputId": "2944acbb-14d0-4390-955d-236ef3a875b3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss)\n",
        "#plt.plot(test_losses)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f08869ab550>]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwV5b3H8c8vK0uULQGRxQCiAiqLEURxR2SxWrtdsVXrLaW3ra22vbfFvW7Va1trra1KFa29brVqa10BV9zQoCibLAUUECFsAQIkJPndP84QT/ZDcpLJOef7fr3O68w8zzMzv4GT35kz88w85u6IiEjySgs7ABERaVlK9CIiSU6JXkQkySnRi4gkOSV6EZEklxF2AHXJzc31/Pz8sMMQEUkY8+bN2+TueXXVtclEn5+fT2FhYdhhiIgkDDP7pL66Rk/dmFkfM3vFzBab2SIzu7SONt80s4/MbIGZvWVmQ6PqVgfl881M2VtEpJXFckRfDvzM3d83swOAeWY2y90XR7VZBZzs7lvNbAIwHRgVVX+qu2+KX9giIhKrRhO9u68H1gfTO8xsCdALWBzV5q2oRd4Besc5ThERaaL96nVjZvnAcGBuA82+AzwfNe/ATDObZ2ZTG1j3VDMrNLPCoqKi/QlLREQaEPPFWDPLAZ4ALnP37fW0OZVIoh8TVTzG3deZWXdglpl97O6v11zW3acTOeVDQUGBHsAjIhInMR3Rm1kmkST/kLs/WU+bo4F7gXPcffO+cndfF7xvBJ4CRjY3aBERiV0svW4MuA9Y4u631dOmL/AkcIG7L4sq7xhcwMXMOgLjgIXxCFxERGITy6mbE4ALgAVmNj8ouwLoC+DudwPXAN2AP0W+Fyh39wKgB/BUUJYBPOzuL8R1D6I8PPdTlm3YwZL125m7aktV+ZLrx9M+K72lNisi0qZZW3wefUFBge/vDVN79lZwxNX1f4esvmVSc8MSEWmzzGxecIBdS5u8M7Yp2mWms/j6M1lfvIeOWRks3bCDrSVlXPbY/MYXFhFJYkn1ULMOWRkMyMvhoE7tOPmwPL48vFdV3Zzl6rIpIqkpqRJ9Q15bqkQvIqkpZRL9vW+sCjsEEZFQpEyiFxFJVUmf6AsO6RJ2CCIioUr6RD+yX9ewQxARCVXSJ/oD2mWGHYKISKiSPtFPOPKgsEMQEQlV0if6/NyOYYcgIhKqpE/0IiKpToleRCTJKdGLiCQ5JXoRkSSnRC8ikuRSKtHvLqsIOwQRkVYXy1CCfczsFTNbbGaLzOzSOtqYmd1hZivM7CMzGxFVd5GZLQ9eF8V7B/bH4vXFYW5eRCQUsQw8Ug78zN3fD8Z/nWdms9x9cVSbCcDA4DUKuAsYZWZdgWuBAsCDZZ92961x3YsYtcHBtEREWlyjR/Tuvt7d3w+mdwBLgF41mp0DPOgR7wCdzawncCYwy923BMl9FjA+rnuwH575aH1YmxYRCc1+naM3s3xgODC3RlUvYE3U/NqgrL7yutY91cwKzaywqKhlBgl54K3VLbJeEZG2LOZEb2Y5wBPAZe6+Pd6BuPt0dy9w94K8vLx4r15EJGXFlOjNLJNIkn/I3Z+so8k6oE/UfO+grL5yERFpJbH0ujHgPmCJu99WT7OngQuD3jfHAcXuvh54ERhnZl3MrAswLigTEZFWEkuvmxOAC4AFZjY/KLsC6Avg7ncDzwETgRXALuDioG6Lmd0AvBcsd727b4lf+CIi0phGE727vwFYI20c+GE9dTOAGU2KTkREmi0l7ow97YjuYYcgIhKalEj0F4w+JOwQRERCkxKJPis9JXZTRKROKZEBM9IavMQgIpLUUiLR9ziwXdghiIiEJiUSfZcOWWGHICISmpRI9JYSeykiUreUSIEHtssMOwQRkdCkRKIXEUllSvQiIklOiV5EJMkp0YuIJLmUS/Tb9+wNOwQRkVaVcol+xcadYYcgItKqUi7Rv7xkY9ghiIi0qpRL9K8uU6IXkdTS6MAjZjYDOAvY6O5H1lH/P8A3o9Y3CMgLRpdaDewAKoBydy+IV+BNtXBd3Mc1FxFp02I5on8AGF9fpbv/2t2Hufsw4HLgtRrDBZ4a1Iee5EVEUlGjid7dXwdiHed1MvBIsyISEZG4its5ejPrQOTI/4moYgdmmtk8M5sar22JiEjsGj1Hvx++BLxZ47TNGHdfZ2bdgVlm9nHwC6GW4ItgKkDfvn3jGJaISGqLZ6+b86hx2sbd1wXvG4GngJH1Lezu0929wN0L8vLy4hiWiEhqi0uiN7NOwMnAP6PKOprZAfumgXHAwnhsT0REYhdL98pHgFOAXDNbC1wLZAK4+91Bs3OBme5eErVoD+ApM9u3nYfd/YX4hS4iIrFoNNG7++QY2jxApBtmdNlKYGhTAxMRkfhImTtjD9IA4SKSolIm0Y8ZmBt2CCIioUiZRJ9mYUcgIhKOlEn0g3oeGHYIIiKhSJlEf2x+17BDEBEJRcok+iMOOiDsEEREQpEyiT4jPWV2VUSkGmU/EZEkl5KJftmGHWGHICLSalIy0c9ZvinsEEREWk1KJvqbnl0cdggiIq0mJRN9pYcdgYhI60nJRC8ikkqU6EVEkpwSvYhIklOiFxFJckr0IiJJrtFEb2YzzGyjmdU53quZnWJmxWY2P3hdE1U33syWmtkKM5sWz8BFRCQ2sRzRPwCMb6TNHHcfFryuBzCzdOCPwARgMDDZzAY3J9jmGjuoR5ibFxEJRaOJ3t1fB7Y0Yd0jgRXuvtLdy4BHgXOasJ646dwhM8zNi4iEIl7n6Eeb2Ydm9ryZDQnKegFrotqsDcrqZGZTzazQzAqLioriFFZ12Rm6JCEiqSceme994BB3Hwr8AfhHU1bi7tPdvcDdC/Ly8uIQVm3nDKv3e0ZEJGk1O9G7+3Z33xlMPwdkmlkusA7oE9W0d1AWmvaZ6WFuXkQkFM1O9GZ2kJlZMD0yWOdm4D1goJn1M7Ms4Dzg6eZurznSdOZGRFJQRmMNzOwR4BQg18zWAtcCmQDufjfwNeD7ZlYO7AbOc3cHys3sEuBFIB2Y4e6LWmQvYjQgL6dq2t0Jvp9ERJJao4ne3Sc3Un8ncGc9dc8BzzUttPhrF3XqpvCTrRowXERSQsqezCgrrww7BBGRVpGyiX7HnvKwQxARaRUpm+jvnbMy7BBERFpFyiZ6EZFUkbKJfm+FztGLSGpI2US/Z68SvYikhpRN9Es37Ag7BBGRVpGyiV5EJFUo0YuIJDklehGRJKdELyKS5FI60e/ZWxF2CCIiLS7lEv3Uk/pXTW/dVRZiJCIirSPlEv2ZQw6qmk7TY4pFJAWkXKJPT/siue8s1YPNRCT5pV6ijzqKP/23r4UYiYhI62g00ZvZDDPbaGYL66n/ppl9ZGYLzOwtMxsaVbc6KJ9vZoXxDLyp+nbrEHYIIiKtKpYj+geA8Q3UrwJOdvejgBuA6TXqT3X3Ye5e0LQQ46tT+8ywQxARaVWxDCX4upnlN1D/VtTsO0Dv5oclIiLxEu9z9N8Bno+ad2Cmmc0zs6kNLWhmU82s0MwKi4qK4hyWiEjqavSIPlZmdiqRRD8mqniMu68zs+7ALDP72N1fr2t5d59OcNqnoKDA4xWXiEiqi8sRvZkdDdwLnOPum/eVu/u64H0j8BQwMh7bExGR2DU70ZtZX+BJ4AJ3XxZV3tHMDtg3DYwD6uy5IyIiLafRUzdm9ghwCpBrZmuBa4FMAHe/G7gG6Ab8ySJ91MuDHjY9gKeCsgzgYXd/oQX2oVl2l1XQPis97DBERFpMLL1uJjdSPwWYUkf5SmBo7SXalhcXfc6Xh/cKOwwRkRaTcnfGApx3bJ+q6csemx9iJCIiLS8lE/3Eo3qGHYKISKtJyUR//IBuYYcgItJqUjLRRz/BUkQk2aVkorcaz6HfW1EZUiQiIi0vJRN9TVc8uSDsEEREWowSPfD4vLWUleuoXkSSkxJ94MIZc8MOQUSkRSjRB95ZuSXsEEREWkTKJvoBeR3DDkFEpFWkbKK/dOxhYYcgItIqUjbR98/VEb2IpIaUTfRH9uoUdggiIq0iZRO9iEiqUKIXEUlySvRRNu8sDTsEEZG4iynRm9kMM9toZnUOBWgRd5jZCjP7yMxGRNVdZGbLg9dF8Qo8Hs6tMeDIMTfODikSEZGWE+sR/QPA+AbqJwADg9dU4C4AM+tKZOjBUUQGBr/WzLo0Ndh4+6+TB4QdgohIi4sp0bv760BDt46eAzzoEe8Anc2sJ3AmMMvdt7j7VmAWDX9htKrDeuTUKluxcUcIkYiItJx4naPvBayJml8blNVXXouZTTWzQjMrLCoqilNYDav5uGKAB95a3SrbFhFpLW3mYqy7T3f3AncvyMvLCy2O91ZtDW3bIiItIV6Jfh3QJ2q+d1BWX3mbtXTDDlZtKgk7DBGRuIlXon8auDDofXMcUOzu64EXgXFm1iW4CDsuKGsz5vz81Fplp/7m1dYPRESkhWTE0sjMHgFOAXLNbC2RnjSZAO5+N/AcMBFYAewCLg7qtpjZDcB7waqud/c29Tzg3l3ahx2CiEiLiinRu/vkRuod+GE9dTOAGfsfWuuo64KsiEgyaTMXY9uaktJyyjVouIgkASV6YFifzrXKhlz7Ij9+9IMQohERiS8leuDC0YfUWf7cgs9bORIRkfhTogcO63FAvXW7yspbMRIRkfhTogf6dO1Qb93ga17k1aUbWzEaEZH4UqIHMtIa7nnz1r83t1IkIiLxp0QPdMyOqZepiEhCUqKPwfTXV7KlpCzsMEREmkSJPka/mbk07BBERJpEiT4weWTfBusfnvspd7y0vJWiERGJHyX6wM1fOYrVt0xqsM1ts5a1UjQiIvGjRC8ikuSU6EVEkpwSfQ2zf3pyg/X3zlnJb15cSlm5HngmIolBHchrOLR77QHDo9347BIAuh+YzYWj81shIhGR5tERfRPN+0Rjy4pIYlCib6J/zv+M/GnPEhlzRUSk7Yop0ZvZeDNbamYrzGxaHfW/M7P5wWuZmW2LqquIqns6nsG3lHlXjY25rUaoEpG2rtFEb2bpwB+BCcBgYLKZDY5u4+4/cfdh7j4M+APwZFT17n117n52HGNvMd1ysvnZGYfF1PbEW1/mtplLdWQvIm1WLEf0I4EV7r7S3cuAR4FzGmg/GXgkHsGF6UenD+TlnzXcAwdgzZbd3PHyChau294KUYmI7L9YEn0vYE3U/NqgrBYzOwToB7wcVdzOzArN7B0z+3J9GzGzqUG7wqKiohjCann98xrugRPt7ZWbWjASEZGmi/fF2POAv7t7RVTZIe5eAJwP3G5mA+pa0N2nu3uBuxfk5eXFOaymi/V8/a+e+5gpf3mvhaMREdl/sST6dUCfqPneQVldzqPGaRt3Xxe8rwReBYbvd5Qh6paTHXPb2Us2smLjDgB2l1Ww6LNinbsXkdDFkujfAwaaWT8zyyKSzGv1njGzI4AuwNtRZV3MLDuYzgVOABbHI/C2auxtr7Nxxx6+85f3mHTHG9zywsdhhyQiKa7RRO/u5cAlwIvAEuBv7r7IzK43s+heNOcBj3r1Q9hBQKGZfQi8Atzi7gmX6I8f0G2/2v/87x9VDT/48NxPWyIkEZGYWVs8tVBQUOCFhYVhh1FN/rRnm7zs9AuOoVeX9gw5uFMcIxIR+YKZzQuuh9aiO2NjdO2XBjfeqB5T/zqPSXe8Qf60Z/UwNBFpdUr0Mbr4hH7c+rWjm72e8be/zmfbdschIhGR2CjR74dvFPRpvFEjVm4q4fhbXq5WduMzi/n9bA1TKCItQ4l+P9107pFxWU9peQWrNpUAcO8bq/jdbA1TKCItQ4l+P31z1CGNji0bi8OveoFTf/MqG7fviUNUIiL1U6Jvov15wmVDRv7qparpv7y1mtmLN8RlvSIi+yjRN1G3nGyumjQoruu89ulFTHmwkBufWcz4219n885SAL77YCGjfjWbispIV9gXFn7OzEWfx3XbIpK81I++GT7dvIuTfv1KaNsvvGosufvxiAYRSV7qR99C+nbrEJfz9U1VcONs3lqhp2aKSMOU6OPgyonxPYWzP86/d25o2xaRxKBEHwffPak/q26eGNr29+ytaLyRiKQsJfo4CXPs2Cffr++p0SIiSvRJ4YqnFoQdgoi0YUr0LeCNX5wadggiIlWU6FtA7y6R3jiTju4ZdigiIkr0LSk7I/LPe+npA7nrmyNCjkZEUlVMid7MxpvZUjNbYWbT6qj/tpkVmdn84DUlqu4iM1sevC6KZ/BtzcNTRnHZ2IFV81dMHMR5x/bhB6cOYMJRPVl244QQoxORVNXonbFmlg4sA84A1hIZQ3Zy9JCAZvZtoMDdL6mxbFegECgAHJgHHOPuWxvaZqLcGdsUj777KX995xMWfbY9rut99sdjNIKVSApr7p2xI4EV7r7S3cuAR4FzYtz2mcAsd98SJPdZwPgYl01K543sy7M/PjHu6510xxtc+dQCinaUsmPP3jrbbK+nXESSW0YMbXoBa6Lm1wKj6mj3VTM7icjR/0/cfU09y/aqayNmNhWYCtC3b98Ywkpsb19+GqNvjgxActnYgRzSrQML1m5nxpurmrzOh+Z+ykPBYOS9u7Rn7dbISFZ3nj+cnp3a8dW73uaeC47hzCEHVS1z/b8Wc8bgHoyuYwD0Ddv3sGzDDk4cmAfAvl9/++4Z2LSzlMz0NDq1z6xaprS8goy0NNLTYruvoKS0nDQz2melN2GPRSQW8boY+y8g392PJnLU/pf9XYG7T3f3AncvyMvLi1NYbVeHzC++Y084NJdzh/fmykmD+P4pA+Ky/n1JHuCShz/gq3e9DcDjhWvYUlLGwnXF7CwtZ8abq5j853cA2F1WwWvLiliyPnJaadSvXuKC+96tWs8fX1lBv8ufY1dZObMWb6DgxtmMvGl2te0eftULfKuOxzKs2bKLvRW1x8sdcu2LDLrmhZj369F3PyV/2rOUlutuYJFYxXJEvw6IHkOvd1BWxd03R83eC9watewpNZZ9dX+DTEYdsyNHsP3zOlJwSBcA0tOMX4w/grGDuvPVu97mqkmDmHJif+5/cxXX/WtxQ6uL2ewlGxlxw6xa5Rt37GHkTV88G3/BL8dVTedPe5azhx7M0x9+Fmm7vZTvPhi5hlJaXsn8Ndt49N1PueS0QwF4e+Vm1m7dRe8uHQD4xj1v8+6qLQCMOTSXc4f34uTD88jK+OI4Y0tJGXv2VtAhK52O2Rmkm5FWx6+CW19cCsCOPeVk53zxK2BvRSVX/2MhPzp9IL06t4/532NrSRl7KyrpfmC7RtsuXFfMrMUb+MkZh1WVrdu2G3ev2tem2vclmJmeRv60Z/nKiF7c9o1hzVqnyD6xXIzNIHI65nQiifs94Hx3XxTVpqe7rw+mzwV+4e7HBRdj5wH7+ha+T+Ri7JaGtpnMF2NjtbJoJ/1yO2JmfF68h+NufomTD8vjgYuPBeBPr/6bXwdJr626cuIgbnpuSZOXX3bjBD5cu43LHp3PVZMGMeGongy9bibFu/fywdVn0D4rnTQzsjLSeG1ZERfNeJcTB+by1++M4rNtu+lxYDvS04wde/ZSWQmZGcaqTSUMObgTxbv2smBdMd+6L/LrY/Utk1i3bTd/eWs108YfUfUlU15RSUZ6GrMXb2BK8OW26uaJmBlL1m9nwu/nVC0P8MGnW/m/dz7lt98YWmt/inftpVOHzFrlAEde+yIAC687k/xpzwLw8Q3jaZepU1oSm4Yuxsb0PHozmwjcDqQDM9z9JjO7Hih096fN7GbgbKAc2AJ8390/Dpb9T+CKYFU3ufv9jW1PiT42W0vKGF7H0Xkq+PDacQy9biYHZGew4LozeX1ZERfOeJcxh+Zy5pEHcfU/FtIuM433rhzLUb+cWW3ZW75yFNOebPixEa/+9yk88f5a/vDyCjq1z6R49xcXsk85PI9XlxZVa5+bk8WZQw6qukZSeNVYCm6czWlHdKdoRykL1hUDYAbH9evGI1OPo6y8kg3b97Bm6y7O/3PkC+fh746qmu7dpT1v/OK05v1DScpodqJvbUr0sdlbUcnAK58PO4xQPPLd46quLcy7aizH3Di7kSUS06Hdcziuf1fOGdaLY/O7hh2OtGFK9EkqlRN9KgpzkBtp+zTClEgSaIsHZZIYlOgTmP7uU0t0l1mR/aFEn8Ay08Mb7ERa3/w12wCorHQKbpzF44VrGllCJEKJPoGZGYd0a17/bUkcP3rkAwDKKirZtLOMq/6xsFnrm79mG7fNWhaP0KSNU6JPcI//1+iwQ5BWtOiz4qqunaXllewqKwfgzpeXs2zDDj4v3sPvZi2rdj6/tLyiqh1EfhGs3bqLL//xTe54aXmLxFm8ey+3z15GRWX95xcrKp0/v76y1cc83lVWTll57bu0k5l63SSBB95cxalHdOe2Wcv45/zPwg5HWll0v/6uHbPYUlLGd8b04+qzBrO3opIJv5/Dio07AXjxspOYvWRDtZvtrpo0iDQzPlq7jYtP6EfRjlJ+8th8BnTP4eIT8tlSUsZJh+UxIC+n1rbXbNlF9wOzyc6I3NhVVl7Jyx9vZNbiDTzx/lr65XbkGwV9qh7tce+clXQ/sB1nDz2YxwvX8D9//4j/PKEf13xpcJ379vyC9ZSWV/Ll4XU+IqtBeysqyUyvfiy7ZssuTrz1FYb16cw/fnhCtbrPtu0mNye76o5td+f5hZ9z/IBuLN+4s813b1X3yhSzsmgnB7TL5NibZjO8b2d++/Wh5LTLoF1mOkfXuHlIZH99eM04NpeU8tKSjVV3Pv9k7GFMPOogzvjd63Uuc+LAXOYs31Q1f9HoQyjaWcpzCz4H4GvH9GZkv66cPfRg1m3bTbvMdHp1bl91l/DqWyaxs7Sc8//8Dn+YPJy+XTuwuaSM3JzsOrc3c9HnTP3rPJ6/9ES65WTx4ZpicrIzqu69APjt14fy1WN6A7BnbwVHXP0C5ww7mOvOHkLnDlm8sPBz/uv/5lW1v//iYzn18O71/ruUlJbzrw8/4z+O7UOlw7xPtnLDM4t57HvH0SGr7qfNRO9fZfDrp65Hf8RCiT5Fvf/pVgbk5VR7uuQ+lz76QdXR/2NTj+M/pr9D5w6ZDDn4QK6YOIhJd7xRrf19FxVw+qAe3DtnJTc+G/njnvWTk2r9YU8Z049732j6Ezi/M6Yf9zVjeUlOuTnZbNpZWjX/9WN68/i8tXTukMl3T+xf9Qvl/FF9OWNwDy6+/72Y133SYXn86Zsjqh5DEYtbv3Y0Zw89mCOujjyQ77ZvDOVPr/676pfTV0f05on311a1nzbhCLofkM1P//YhuTlZPDTlOCrdqx6hEa2p90so0UstZeWVFO0srXoA2O6yCtLSqPoJvqWkjKyMNHKyax+JfLZtNyWl5QzscQAAv525lD17K7hyUuTn9+LPtrOiaCflFZV8ZUTkiKm8opIv3fkmI/O78Muzh1BaXln1R/LRL8fxv89/TP+8HC4+Pp//mP42k0f25f43V1c9OqBLh0y27grnefpnHd2TZz5aX63shi8fydUNXAz93kn9GTekR9VTQ0VipUQvSWXuys28/+m2Rh/N/HjhGkb168Zdr63g30UlVU/CrOmMwT2447zhtM9K5+bnl3DPayt594rTwaj2ZM63pp1Gp/aZdAy+xMrKK/nVc0v49vH55Od2xN2Z98lWsjPSObR7Du2z0qt+Yu+z+pZJzF+zDQO6dMhixpureOCt1Vx91mCKd+/lp8ETLvctd8fk4fz4kQ/ol9uRKycO4u2Vm7nvjVUc1asTC9YVM25wD2Yu3gDAoJ4H8vMzD2d98R6ueKrhZ/JI8lGil5RXUekMuOK5WuV3fXMEE47qWa3dxh176Nkp8ovlgvvmMmf5pib/ET349mrunbOKgvwunHp4d7409OBq9e5OSVlFrV9At89expaSMq4/58hq5QvXFXPWH97g4SmjOP7Q3KrHROfmZFN41dhqbX8/ezm/m61ukKmiJRJ9LM+jF2kz0tOM1bdM4tWlG/n2/e8x49sF9O3akUO759Rqty/JA0y/oIAN2/c0ebsXjs7nwtH59dabWZ2nuS4be1gdreHIXp2q/UF3bp8FwH+Pq90+QzfGSTMp0UtCOuXw7nx47bg6LzTXpX1WOvm5HVs4qqbLykir90iua8esVo5Gko1umJKEFWuST3RfP6Y3PQ6suxuhSCyU6EXauIz0NN78xWnkHaBkL00TU6I3s/FmttTMVpjZtDrqf2pmi83sIzN7ycwOiaqrMLP5wevpeAYvkioy0iOjZdVlyMEHtnI0kmgaTfRmlg78EZgADAYmm1nN+5U/AArc/Wjg73wxODjAbncfFrzOjlPcIilt9S2TWH3LJJbfNIFnf3wiH98wnnOH9+LEgbmMHdSd5y89kfeuHMtFoyPHXF07ZvHz8Ydz7vBe9O3agXlXjeWcYQdzxuAetdY96eieTDzqoGpl42q0u+C4yHr3PS7gmrPqfoTB147pzQdXn0GXDpnc+rWjG92viUcdRBNvDE0Kd54/vEXWG8vg4KOBX7r7mcH85QDufnM97YcDd7r7CcH8Tnev/ZCMBqh7pUjdlqzfzoK1xXzj2D4xtS8tr+CGZxbz0zMOb/Ci7qpNJbg7/Ws8z2bHnr1s31NedWPdJ5tLyMnOoFtONm//ezMj+3UlPcjM++4ZOOHQbqzdupuvjujNj08fWG1973+6lZ88Np/X/udU9uytoKLSWV+8h1WbSqp96aws2knnDlmMiBoT+eqzBjP+yINYWbSTpZ/vqLpDe0BeRyYc2ZM7X1kBwHVnD6F/Xkc+Xr+Dm55bwqv/fQolZeXV7va+9kuD2b67vKrb6oqbJnDsTbNpn5nO/00ZxS//tZgpY/pxaPccnpi3lr7dOpCZnsYPHnq/2v48PGUU5987t9H/h++d3J97XltZNX9guwye/fGJTP3rPK4/ZwhfvztyY11zRhFrVj96M/saMN7dpwTzFwCj3P2SetrfCXzu7jcG8+XAfCIDh9/i7v+oZ7mpwFSAvn37HvPJJ5/Esm8i0kYU79pLpTtd4thLaOaiz+mYncGh3XPofkA2Zl8c7q8v3s3om1/murOHcARBMWAAAAYvSURBVNHx+ZSUlrO3opLOHere/vri3eTlZJMR9aCzy59cQGa61brPoTHuTqVHuvE++PZqrvnnIr53cn9OPDSPb903l2u/NJhj87vSMTuDflG9vTbtLK33+TzN1WqJ3sy+BVwCnOzupUFZL3dfZ2b9gZeB09393w1tU0f0IhKLPXsryM5Iq/YF0NpKyyu4beYyfnT6wDrvpWgtzb1hah0Q/Tuxd1BWcyNjgSuJSvIA7r4ueF9pZq8Cw4EGE72ISCzaZaaHHQLZGelcPnFQ2GE0KJZeN+8BA82sn5llAecB1XrPBOfl7wHOdveNUeVdzCw7mM4FTgAWxyt4ERFpXKNH9O5ebmaXAC8C6cAMd19kZtcDhe7+NPBrIAd4PPgJ9WnQw2YQcI+ZVRL5UrnF3ZXoRURakR5qJiKSBBo6R687Y0VEkpwSvYhIklOiFxFJckr0IiJJToleRCTJtcleN2ZWBDT1GQi5wKY4htPaEj1+0D60FYm+D4keP7TuPhzi7nl1VbTJRN8cZlZYXxejRJDo8YP2oa1I9H1I9Pih7eyDTt2IiCQ5JXoRkSSXjIl+etgBNFOixw/ah7Yi0fch0eOHNrIPSXeOXkREqkvGI3oREYmiRC8ikuSSJtGb2XgzW2pmK8xsWhuIZ4aZbTSzhVFlXc1slpktD967BOVmZncEsX9kZiOilrkoaL/czC6KKj/GzBYEy9xhcR5ix8z6mNkrZrbYzBaZ2aUJuA/tzOxdM/sw2IfrgvJ+ZjY32O5jwTgLmFl2ML8iqM+PWtflQflSMzszqrzFP3dmlm5mH5jZMwka/+rg/3m+mRUGZQnzOQq20dnM/m5mH5vZEjMbnVD74O4J/yLynPx/A/2BLOBDYHDIMZ0EjAAWRpXdCkwLpqcB/xtMTwSeBww4DpgblHcFVgbvXYLpLkHdu0FbC5adEOf4ewIjgukDgGXA4ATbBwNygulMYG6wvb8B5wXldwPfD6Z/ANwdTJ8HPBZMDw4+U9lAv+Czlt5anzvgp8DDwDPBfKLFvxrIrVGWMJ+jYBt/AaYE01lA50Tah7j+Y4T1AkYDL0bNXw5c3gbiyqd6ol8K9AymewJLg+l7gMk12wGTgXuiyu8JynoCH0eVV2vXQvvyT+CMRN0HoAPwPjCKyJ2KGTU/O0QG1xkdTGcE7azm52lfu9b43BEZuvMl4DTgmSCehIk/WO9qaif6hPkcAZ2AVQSdVxJxH5Ll1E0vYE3U/NqgrK3p4e7rg+nPgR7BdH3xN1S+to7yFhGcAhhO5Ig4ofYhOO0xH9gIzCJyBLvN3cvr2G5VrEF9MdCtkX1o6c/d7cDPgcpgvluCxQ/gwEwzm2dmU4OyRPoc9QOKgPuDU2j3mlnHRNqHZEn0CccjX91tvm+rmeUATwCXufv26LpE2Ad3r3D3YUSOjEcCR4QcUszM7Cxgo7vPCzuWZhrj7iOACcAPzeyk6MoE+BxlEDkNe5e7DwdKiJyqqdLW9yFZEv06oE/UfO+grK3ZYGY9AYL3fQOp1xd/Q+W96yiPKzPLJJLkH3L3JxNxH/Zx923AK0ROV3Q2s33jJUdvtyrWoL4TsJn937d4OQE428xWA48SOX3z+wSKHwB3Xxe8bwSeIvKFm0ifo7XAWnefG8z/nUjiT5x9iPf5uDBeRL5xVxL5ibXvotKQNhBXPtXP0f+a6hdvbg2mJ1H94s27QXlXIucGuwSvVUDXoK7mxZuJcY7dgAeB22uUJ9I+5AGdg+n2wBzgLOBxql/M/EEw/UOqX8z8WzA9hOoXM1cSuZDZap874BS+uBibMPEDHYEDoqbfAsYn0uco2MYc4PBg+pdB/AmzD3H/QIb1InKlexmRc7BXtoF4HgHWA3uJHBF8h8j50peA5cDsqP9kA/4YxL4AKIhaz38CK4LXxVHlBcDCYJk7qXGhKA7xjyHyU/QjYH7wmphg+3A08EGwDwuBa4Ly/sEf1goiSTM7KG8XzK8I6vtHrevKIM6lRPWIaK3PHdUTfcLEH8T6YfBatG8bifQ5CrYxDCgMPkv/IJKoE2Yf9AgEEZEklyzn6EVEpB5K9CIiSU6JXkQkySnRi4gkOSV6EZEkp0QvIpLklOhFRJLc/wP21IYoAHqCeAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Archived codes"
      ],
      "metadata": {
        "id": "Dze3P0J4fC7l"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbvQWh1jVcI"
      },
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(net, dataloader, optimizer, epoch=1, verbose=True):\n",
        "  net.to(device)\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  sum_loss = 0.0\n",
        "  for i, batch in enumerate(dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize \n",
        "    outputs = net(inputs)\n",
        "    '''\n",
        "    y = torch.zeros([outputs.shape[0], 1, 9, 9])\n",
        "    for k in range(outputs.shape[0]):\n",
        "      for i in range(9):\n",
        "        for j in range(9):\n",
        "          y[k, 0, i, j] = torch.argmax(outputs[k, :, i, j])+1\n",
        "          print(y[k, 0, i, j])\n",
        "    '''\n",
        "    print(\"INPUT\")\n",
        "    #print(inputs)\n",
        "    #print(\"LABEL\")\n",
        "    #print(labels)\n",
        "    #print(\"OUTPUT\")\n",
        "    #print(outputs)\n",
        "    # break\n",
        "    \n",
        "    loss = criterion(outputs, torch.flatten(labels, 1))\n",
        "    print(loss)\n",
        "    break\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    losses.append(loss.item())\n",
        "    sum_loss += loss.item()\n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "      if verbose:\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch, i + 1, sum_loss / 100))\n",
        "      sum_loss = 0.0\n",
        "    \n",
        "  return losses\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    blank_cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, label) in enumerate(test_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        output = model(data)\n",
        "        \n",
        "        '''\n",
        "        blank_mask = torch.zeros([output.shape[0], 1, 9, 9]).floats()\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              is_blank = True\n",
        "              for n in range(9):\n",
        "                if data[k, n, i, j] != 0:\n",
        "                  is_blank = False\n",
        "                  break;\n",
        "              if is_blank:\n",
        "                blank_mask[k, 0, i, k] = 1.0\n",
        "                blank_cnt += 1\n",
        "          '''\n",
        "        \n",
        "        x = torch.zeros([output.shape[0], 1, 9, 9])\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              x[k, 0, i, j] = torch.argmax(output[k, :, i, j])+1\n",
        "        \n",
        "        y = torch.zeros([label.shape[0], 1, 9, 9])\n",
        "        for k in range(label.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              y[k, 0, i, j] = torch.argmax(label[k, :, i, j])+1\n",
        "\n",
        "        correct_mask = x.eq(y.view_as(x))\n",
        "\n",
        "        num_correct = (correct_mask).sum().item()\n",
        "\n",
        "        correct += num_correct\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) /81\n",
        "    #test_accuracy = 100. * correct / blank_cnt\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "Sm78W6f5jZUR",
        "outputId": "9c54a735-da04-4d18-a491-24695947e6ec"
      },
      "source": [
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 1\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0\n",
        "GAMMA = 0.9\n",
        "DATA_PATH = \"/gdrive/MyDrive/490g1/checkpoints/sudokuNetChannel\"\n",
        "\n",
        "EXPERIMENT_VERSION = \"0.1\" # increment this to start a new experiment\n",
        "\n",
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "#torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Simple sudokuNet\n",
        "Parameters:\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0\n",
        "GAMMA = 0.9\n",
        "DATA_PATH = \"checkpoints/sudokuNet\"\n",
        "\"\"\"\n",
        "# model = sudokuNet().to(device)\n",
        "# train_loader = DataLoader(sudokuDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "# test_loader = DataLoader(sudokuDataset(test_data), batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "9-Channel sudokuNet\n",
        "\"\"\"\n",
        "model = sudokuNetChannel().to(device)\n",
        "train_loader = DataLoader(sudokuDatasetChannel(train_data), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(sudokuDatasetChannel(test_data), batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "start_epoch = 1\n",
        "if os.listdir(DATA_PATH):\n",
        " start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        " print(f\"loaded from {DATA_PATH}, starting from epoch {start_epoch}\")\n",
        "\n",
        "lr = LEARNING_RATE * (np.power(GAMMA, start_epoch))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, GAMMA)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "\"\"\"\n",
        "for epoch in range(start_epoch, EPOCHS+1):\n",
        "  print(f\"CURRENT EPOCH: {epoch}\")\n",
        "  train_loss = train(model, train_loader, optimizer, epoch-1)\n",
        "  _, accuracy = test(model, test_loader)\n",
        "  losses.extend(train_loss)\n",
        "  # accuracies.append(accuracy)\n",
        "  model.save_best_model(accuracy, DATA_PATH+f\"/epoch{epoch}.pt\")\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n",
            "num cpus: 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c90499b4ff3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m  \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_last_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loaded from {DATA_PATH}, starting from epoch {start_epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/MyDrive/490g1/checkpoints/sudokuNetChannel'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = 'cuda' if USE_CUDA else 'cpu'\n",
        "\n",
        "class SudokuStepNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SudokuStepNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(9, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(16, 9, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9 * 9 * 9, 9 * 9 * 9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, label, mask):\n",
        "        x = F.softmax(x, dim=1)\n",
        "        x = x * mask\n",
        "        chosen = torch.argmax(x, dim=1)\n",
        "        loss = torch.tensor(0., device=DEVICE)\n",
        "\n",
        "        for n in range(x.size(dim=0)):\n",
        "            k = chosen[n] // 81\n",
        "            pos = chosen[n] % 81\n",
        "            if label[n, pos] + 1 != k:\n",
        "                loss += x[n, chosen[n]]\n",
        "            else:\n",
        "                loss += (1 - x[n, chosen[n]])\n",
        "\n",
        "        return loss, chosen\n",
        "\n",
        "\n",
        "class SudokuStepDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed_feat = np.zeros((9, 9, 9))\n",
        "\n",
        "        for i in range(9):\n",
        "            for j in range(9):\n",
        "                if feat[0, i, j]:\n",
        "                    processed_feat[feat[0, i, j] - 1, i, j] = 1\n",
        "\n",
        "        return (torch.from_numpy(processed_feat).float(), torch.from_numpy(label).float().flatten())\n",
        "\n",
        "\n",
        "\n",
        "def train_step(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0, verbose=1):\n",
        "    net.to(DEVICE)\n",
        "    losses = []\n",
        "    # criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "    for epoch in range(epochs):\n",
        "        sum_loss = 0.0\n",
        "        for i, batch in enumerate(dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = batch[0].to(DEVICE), batch[1].to(DEVICE)\n",
        "            used_mask = torch.ones(size=(inputs.size(dim=0), 9*9*9), dtype=torch.float16, device=DEVICE)\n",
        "\n",
        "            for _ in range(9*9):\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                outputs = net(inputs)\n",
        "\n",
        "                # mask selected portions\n",
        "                loss, chosen = net.loss(outputs, labels, used_mask)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                for n in range(inputs.size(dim=0)):\n",
        "                    chosen_num = chosen[n] // 81 + 1\n",
        "                    chosen_pos = chosen[n] % 81\n",
        "                    for num in range(9):\n",
        "                        used_mask[n, chosen_pos + num*81] = 0\n",
        "                    inputs[n, int(labels[n, chosen_pos]) - 1, chosen_pos // 9, chosen_pos % 9] = 1\n",
        "\n",
        "                print(loss.item())\n",
        "                # print statistics\n",
        "                losses.append(loss.item())\n",
        "                sum_loss += loss.item()\n",
        "        if i % 100 == 99:  # print every 100 mini-batches\n",
        "            if verbose:\n",
        "                print('[%d, %5d] loss: %.5f' %\n",
        "                      (epoch + 1, i + 1, sum_loss / 100))\n",
        "            sum_loss = 0.0\n",
        "    return losses\n",
        "\n",
        "def test_step(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct_cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, label) in enumerate(test_loader):\n",
        "            data, label = data.to(DEVICE), label.to(DEVICE)\n",
        "\n",
        "            used_mask = torch.ones(size=(data.size(dim=0), 9*9*9), dtype=torch.float16, device=DEVICE)\n",
        "\n",
        "            for _ in range(9 * 9):\n",
        "                x = model(data)\n",
        "                chosen = model.chosen(x, used_mask)\n",
        "                for n in range(data.size(dim=0)):\n",
        "                    chosen_num = chosen[n] // 81\n",
        "                    chosen_pos = chosen[n] % 81\n",
        "                    chosen_i = chosen_pos // 9\n",
        "                    chosen_j = chosen_pos % 9\n",
        "                    for num in range(9):\n",
        "                        if data[n, num, chosen_i, chosen_j]:\n",
        "                            chosen_num = num\n",
        "                    for num in range(9):\n",
        "                        used_mask[n, chosen_pos + num * 81] = 0\n",
        "                    data[n, chosen_num, chosen_pos // 9, chosen_pos % 9] = 1\n",
        "\n",
        "            outputs = torch.zeros(label.size()).to(DEVICE)\n",
        "            for n in range(data.size(dim=0)):\n",
        "                for pos in range(9*9):\n",
        "                    for chosen in range(9):\n",
        "                        if data[n, chosen, pos // 9, pos % 9] == 1:\n",
        "                            outputs[n, pos] = chosen + 1\n",
        "\n",
        "            correct = outputs.eq(label)\n",
        "            correct_cnt += correct.sum().item()\n",
        "\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) / 81\n",
        "    print(f'accuracy: {test_accuracy}, correct: {correct_cnt} / {len(test_loader.dataset) * 81}')\n",
        "\n"
      ],
      "metadata": {
        "id": "x8hrJ6wwK2DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "TEST_BATCH_SIZE = 10\n",
        "\n",
        "\n",
        "train_dataset_step = SudokuStepDataset(train_data)\n",
        "test_dataset_step = SudokuStepDataset(test_data)\n",
        "print(train_dataset_step[10])\n",
        "train_data_loader_step = DataLoader(train_dataset_step, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_data_loader_step = DataLoader(test_dataset_step, batch_size=TEST_BATCH_SIZE, shuffle=TEST_BATCH_SIZE)\n",
        "\n",
        "\n",
        "model = SudokuStepNet()\n",
        "\n",
        "train_loss = []\n",
        "train_loss += train_step(model, train_data_loader_step, lr=0.1)\n",
        "train_loss += train_step(model, train_data_loader_step, lr=0.05)\n",
        "train_loss += train_step(model, train_data_loader_step, lr=0.05)\n",
        "train_loss += train_step(model, train_data_loader_step, lr=0.01, momentum=0.5)\n",
        "train_loss += train_step(model, train_data_loader_step, lr=0.01, momentum=0.5)\n",
        "\n",
        "torch.save(model.state_dict(), \"/gdrive/MyDrive/490g1/checkpoints/step.pt\")\n",
        "\n",
        "test_step(model, test_data_loader_step)"
      ],
      "metadata": {
        "id": "FkSMy-UpLoqe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}