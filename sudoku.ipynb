{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sudoku.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Toa8bOcSbAoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0279a63-2e79-4159-f9b2-68f96705f0ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive/')\n",
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive/\n",
            "MyDrive  Shareddrives\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3dYUF5zbn2H"
      },
      "source": [
        "import os\n",
        "! pip install -q kaggle\n",
        "\n",
        "if not os.path.exists('/content/sudoku.csv'):\n",
        "  if not os.path.exists('~/.kaggle'):\n",
        "    ! mkdir ~/.kaggle\n",
        "  ! cp /gdrive/MyDrive/kaggle.json ~/.kaggle/\n",
        "  ! chmod 600 ~/.kaggle/kaggle.json\n",
        "  ! kaggle datasets download -d bryanpark/sudoku\n",
        "  ! unzip -a sudoku.zip\n",
        "\n",
        "if not os.path.exists('/content/pt_util.py'):\n",
        " ! cp /gdrive/MyDrive/pt_util.py /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYQxjeohfYbi",
        "outputId": "ee5cc93a-11f8-43d2-b56b-64e086f4d2ec"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_data  sudoku.csv  sudoku.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOk7_-5HgP3I"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import h5py\n",
        "import sys\n",
        "import pandas as pd\n",
        "import pt_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6fjc0wgU1j"
      },
      "source": [
        "quizzes = np.zeros((1000000, 81), np.int32)\n",
        "solutions = np.zeros((1000000, 81), np.int32)\n",
        "data = np.zeros((1000000, 2, 81), np.int32)\n",
        "for i, line in enumerate(open('sudoku.csv', 'r').read().splitlines()[1:]):\n",
        "    quiz, solution = line.split(\",\")\n",
        "    for j, q_s in enumerate(zip(quiz, solution)):\n",
        "        q, s = q_s\n",
        "        quizzes[i, j] = q\n",
        "        solutions[i, j] = s\n",
        "        data[i, 0, j] = q\n",
        "        data[i, 1, j] = s\n",
        "quizzes = quizzes.reshape((-1, 9, 9))\n",
        "solutions = solutions.reshape((-1, 9, 9))\n",
        "data = data.reshape((-1, 2, 9, 9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27BnPMSF7Ww2"
      },
      "source": [
        "quizzes = quizzes.reshape((-1, 1, 9, 9))\n",
        "solutions = solutions.reshape((-1, 1, 9, 9))\n",
        "data = data.reshape((-1, 2, 1, 9, 9))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhMyKmHDlhJG",
        "outputId": "da59aff9-0dde-470f-b6ff-9e419901a9a0"
      },
      "source": [
        "print(quizzes.shape)\n",
        "print(solutions.shape)\n",
        "print(data.shape)\n",
        "train_quizzes = quizzes[:800000,:,:]\n",
        "test_quizzes = quizzes[800000:,:,:]\n",
        "train_solutions = solutions[:800000,:,:]\n",
        "test_solutions = solutions[800000:,:,:]\n",
        "train_data = data[:200000,:,:,:]\n",
        "test_data = data[200000:210000,:,:,:]\n",
        "print(train_quizzes.shape)\n",
        "print(test_solutions.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000000, 1, 9, 9)\n",
            "(1000000, 1, 9, 9)\n",
            "(1000000, 2, 1, 9, 9)\n",
            "(800000, 1, 9, 9)\n",
            "(200000, 1, 9, 9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqLKQQ1BPFpl",
        "outputId": "50108abc-0899-4b92-fa4f-130addfa2b31"
      },
      "source": [
        "print(train_data[0,0,:,:])\n",
        "print(train_data[0,1,:,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 0 4 3 0 0 2 0 9]\n",
            "  [0 0 5 0 0 9 0 0 1]\n",
            "  [0 7 0 0 6 0 0 4 3]\n",
            "  [0 0 6 0 0 2 0 8 7]\n",
            "  [1 9 0 0 0 7 4 0 0]\n",
            "  [0 5 0 0 8 3 0 0 0]\n",
            "  [6 0 0 0 0 0 1 0 5]\n",
            "  [0 0 3 5 0 8 6 9 0]\n",
            "  [0 4 2 9 1 0 3 0 0]]]\n",
            "[[[8 6 4 3 7 1 2 5 9]\n",
            "  [3 2 5 8 4 9 7 6 1]\n",
            "  [9 7 1 2 6 5 8 4 3]\n",
            "  [4 3 6 1 9 2 5 8 7]\n",
            "  [1 9 8 6 5 7 4 3 2]\n",
            "  [2 5 7 4 8 3 9 1 6]\n",
            "  [6 8 9 7 3 4 1 2 5]\n",
            "  [7 1 3 5 2 8 6 9 4]\n",
            "  [5 4 2 9 1 6 3 7 8]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCXYY8Fj90ei"
      },
      "source": [
        "# Data loader\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class sudokuDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.transform = transform\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed = np.zeros((9, 9, 9))\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed[label[0, i, j] - 1, i, j] = 1 \n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "        \n",
        "        return (torch.from_numpy(feat).float(), torch.from_numpy(processed).float())\n",
        "\n",
        "class sudokuDatasetChannel(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "      self.transform = transform\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed_label = np.zeros((9, 9, 9))\n",
        "        processed_feat = np.zeros((9, 9, 9))\n",
        "\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed_label[label[0, i, j] - 1, i, j] = 1 \n",
        "            if feat[0, i, j]:\n",
        "              processed_feat[feat[0, i, j] - 1, i, j] = 1\n",
        "        \n",
        "        return (torch.from_numpy(processed_feat).float(), torch.from_numpy(processed_label).float())\n",
        "\n",
        "class sudokuDatasetChannelFlattenLabel(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "      self.transform = transform\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "      \n",
        "    def __getitem__(self, idx):\n",
        "        feat = self.data[idx, 0, :, :]\n",
        "        label = self.data[idx, 1, :, :]\n",
        "        processed_label = np.zeros((9, 9, 9))\n",
        "        processed_feat = np.zeros((9, 9, 9))\n",
        "\n",
        "        for i in range(9):\n",
        "          for j in range(9):\n",
        "            processed_label[label[0, i, j] - 1, i, j] = 1 \n",
        "            if feat[0, i, j]:\n",
        "              processed_feat[feat[0, i, j] - 1, i, j] = 1\n",
        "        \n",
        "        return (torch.from_numpy(processed_feat).float(), torch.flatten(torch.from_numpy(processed_label).float()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6qRHyBTnN2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "443f4c0e-d021-482e-bfae-4310ad28d5a2"
      },
      "source": [
        "train_loader = DataLoader(sudokuDataset(train_data), batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(sudokuDataset(test_data), batch_size=1, shuffle=True)\n",
        "#print(train_loader.dataset.__getitem__(0))\n",
        "\n",
        "train_loader_channel = DataLoader(sudokuDatasetChannel(train_data), batch_size=16, shuffle=True)\n",
        "test_loader_channel = DataLoader(sudokuDatasetChannel(test_data), batch_size=1, shuffle=True)\n",
        "print(train_loader_channel.dataset.__getitem__(0))\n",
        "\n",
        "train_loader_flattern = DataLoader(sudokuDatasetChannelFlattenLabel(train_data), batch_size=16, shuffle=True)\n",
        "test_loader_flattern = DataLoader(sudokuDatasetChannelFlattenLabel(test_data), batch_size=1, shuffle=True)\n",
        "print(train_loader_flattern.dataset.__getitem__(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.]]]), tensor([[[0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.]],\n",
            "\n",
            "        [[1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.]]]))\n",
            "(tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "\n",
            "        [[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
            "         [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "         [0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "         [0., 0., 0., 1., 0., 0., 0., 0., 0.]]]), tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0.]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLdPj1Ep6WlF"
      },
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, inputs=9*9, hidden=512, outputs=(9*9)):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(inputs, hidden)\n",
        "        self.fc2 = nn.Linear(hidden, outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADe3SpNBjUwY"
      },
      "source": [
        "\n",
        "! mkdir checkpoints\n",
        "file_path = \"checkpoints\"\n",
        "class sudokuNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sudokuNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 9, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(9, 1, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9*9, 9*9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)\n",
        "\n",
        "\n",
        "class sudokuNetChannel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sudokuNetChannel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(9, 16, 3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 16, 3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(16, 9, 3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(9*9*9, 9*9*9)\n",
        "        self.accuracy = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        #x = F.relu(x)\n",
        "        x = x.reshape((-1, 9, 9 ,9))\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        #x = nn.BatchNorm2d(16)(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        #x = nn.BatchNorm2d(32)(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        #x = nn.BatchNorm2d(16)(x)\n",
        "        x = self.conv4(x)\n",
        "        #x = nn.BatchNorm2d(9)(x)\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "    def loss(self, prediction, label, reduction='mean'):\n",
        "        loss_val = F.cross_entropy(prediction, label.squeeze(), reduction=reduction)\n",
        "        return loss_val\n",
        "\n",
        "    def save_model(self, file_path, num_to_keep=1):\n",
        "        pt_util.save(self, file_path, num_to_keep)\n",
        "        \n",
        "    def save_best_model(self, accuracy, file_path, num_to_keep=1):\n",
        "        if self.accuracy == None or accuracy > self.accuracy:\n",
        "            self.accuracy = accuracy\n",
        "            self.save_model(file_path, num_to_keep)\n",
        "\n",
        "    def load_model(self, file_path):\n",
        "        pt_util.restore(self, file_path)\n",
        "\n",
        "    def load_last_model(self, dir_path):\n",
        "        return pt_util.restore_latest(self, dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, dataloader, epochs=1, lr=0.01, momentum=0.9, decay=0.0, verbose=1):\n",
        "  net.to(device)\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum, weight_decay=decay)\n",
        "  for epoch in range(epochs):\n",
        "    sum_loss = 0.0\n",
        "    for i, batch in enumerate(dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize \n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        losses.append(loss.item())\n",
        "        sum_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 100 mini-batches\n",
        "            if verbose:\n",
        "              print('[%d, %5d] loss: %.5f' %\n",
        "                  (epoch + 1, i + 1, sum_loss / 100))\n",
        "            sum_loss = 0.0\n",
        "  return losses"
      ],
      "metadata": {
        "id": "sQxHDaD2gl4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sudokuNetChannel().to(device)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY, momentum=0.9)\n",
        "train_loss = []\n",
        "train_loss += train(model, train_loader_channel, lr=0.1)\n",
        "train_loss += train(model, train_loader_channel, lr=0.05)\n",
        "train_loss += train(model, train_loader_channel, lr=0.05)\n",
        "train_loss += train(model, train_loader_channel, lr=0.01, momentum=0.5)\n",
        "train_loss += train(model, train_loader_channel, lr=0.01, momentum=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVKOVcyoa3jO",
        "outputId": "37c30c44-921b-47d6-e2e7-076f97d3829b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   100] loss: 2.19731\n",
            "[1,   200] loss: 2.19717\n",
            "[1,   300] loss: 2.19712\n",
            "[1,   400] loss: 2.19706\n",
            "[1,   500] loss: 2.19694\n",
            "[1,   600] loss: 2.19681\n",
            "[1,   700] loss: 2.19641\n",
            "[1,   800] loss: 2.19573\n",
            "[1,   900] loss: 2.19334\n",
            "[1,  1000] loss: 2.17611\n",
            "[1,  1100] loss: 2.10503\n",
            "[1,  1200] loss: 2.00066\n",
            "[1,  1300] loss: 1.87394\n",
            "[1,  1400] loss: 1.75473\n",
            "[1,  1500] loss: 1.65122\n",
            "[1,  1600] loss: 1.55054\n",
            "[1,  1700] loss: 1.45688\n",
            "[1,  1800] loss: 1.37783\n",
            "[1,  1900] loss: 1.30997\n",
            "[1,  2000] loss: 1.25355\n",
            "[1,  2100] loss: 1.20501\n",
            "[1,  2200] loss: 1.16579\n",
            "[1,  2300] loss: 1.12104\n",
            "[1,  2400] loss: 1.09185\n",
            "[1,  2500] loss: 1.05921\n",
            "[1,  2600] loss: 1.03631\n",
            "[1,  2700] loss: 1.00539\n",
            "[1,  2800] loss: 0.98328\n",
            "[1,  2900] loss: 0.96238\n",
            "[1,  3000] loss: 0.94375\n",
            "[1,  3100] loss: 0.92705\n",
            "[1,  3200] loss: 0.90808\n",
            "[1,  3300] loss: 0.89626\n",
            "[1,  3400] loss: 0.87563\n",
            "[1,  3500] loss: 0.85966\n",
            "[1,  3600] loss: 0.85121\n",
            "[1,  3700] loss: 0.83527\n",
            "[1,  3800] loss: 0.82089\n",
            "[1,  3900] loss: 0.81058\n",
            "[1,  4000] loss: 0.79487\n",
            "[1,  4100] loss: 0.79326\n",
            "[1,  4200] loss: 0.78160\n",
            "[1,  4300] loss: 0.77304\n",
            "[1,  4400] loss: 0.76645\n",
            "[1,  4500] loss: 0.75495\n",
            "[1,  4600] loss: 0.74981\n",
            "[1,  4700] loss: 0.73832\n",
            "[1,  4800] loss: 0.73514\n",
            "[1,  4900] loss: 0.72142\n",
            "[1,  5000] loss: 0.72223\n",
            "[1,  5100] loss: 0.71227\n",
            "[1,  5200] loss: 0.71034\n",
            "[1,  5300] loss: 0.70578\n",
            "[1,  5400] loss: 0.69906\n",
            "[1,  5500] loss: 0.69788\n",
            "[1,  5600] loss: 0.69565\n",
            "[1,  5700] loss: 0.68719\n",
            "[1,  5800] loss: 0.68188\n",
            "[1,  5900] loss: 0.68606\n",
            "[1,  6000] loss: 0.67244\n",
            "[1,  6100] loss: 0.66195\n",
            "[1,  6200] loss: 0.66726\n",
            "[1,  6300] loss: 0.65775\n",
            "[1,  6400] loss: 0.64990\n",
            "[1,  6500] loss: 0.65159\n",
            "[1,  6600] loss: 0.65096\n",
            "[1,  6700] loss: 0.64104\n",
            "[1,  6800] loss: 0.64646\n",
            "[1,  6900] loss: 0.63827\n",
            "[1,  7000] loss: 0.63526\n",
            "[1,  7100] loss: 0.62690\n",
            "[1,  7200] loss: 0.62520\n",
            "[1,  7300] loss: 0.62675\n",
            "[1,  7400] loss: 0.61710\n",
            "[1,  7500] loss: 0.62365\n",
            "[1,  7600] loss: 0.61436\n",
            "[1,  7700] loss: 0.61427\n",
            "[1,  7800] loss: 0.60979\n",
            "[1,  7900] loss: 0.60838\n",
            "[1,  8000] loss: 0.60005\n",
            "[1,  8100] loss: 0.59824\n",
            "[1,  8200] loss: 0.59722\n",
            "[1,  8300] loss: 0.60065\n",
            "[1,  8400] loss: 0.59546\n",
            "[1,  8500] loss: 0.58944\n",
            "[1,  8600] loss: 0.58911\n",
            "[1,  8700] loss: 0.59112\n",
            "[1,  8800] loss: 0.58631\n",
            "[1,  8900] loss: 0.58760\n",
            "[1,  9000] loss: 0.58454\n",
            "[1,  9100] loss: 0.58250\n",
            "[1,  9200] loss: 0.58590\n",
            "[1,  9300] loss: 0.57618\n",
            "[1,  9400] loss: 0.58167\n",
            "[1,  9500] loss: 0.57479\n",
            "[1,  9600] loss: 0.57460\n",
            "[1,  9700] loss: 0.57133\n",
            "[1,  9800] loss: 0.56674\n",
            "[1,  9900] loss: 0.57851\n",
            "[1, 10000] loss: 0.56588\n",
            "[1, 10100] loss: 0.56458\n",
            "[1, 10200] loss: 0.56122\n",
            "[1, 10300] loss: 0.56295\n",
            "[1, 10400] loss: 0.55915\n",
            "[1, 10500] loss: 0.55561\n",
            "[1, 10600] loss: 0.56039\n",
            "[1, 10700] loss: 0.55377\n",
            "[1, 10800] loss: 0.55260\n",
            "[1, 10900] loss: 0.55567\n",
            "[1, 11000] loss: 0.55163\n",
            "[1, 11100] loss: 0.54909\n",
            "[1, 11200] loss: 0.54772\n",
            "[1, 11300] loss: 0.54878\n",
            "[1, 11400] loss: 0.54733\n",
            "[1, 11500] loss: 0.54662\n",
            "[1, 11600] loss: 0.54284\n",
            "[1, 11700] loss: 0.54858\n",
            "[1, 11800] loss: 0.54070\n",
            "[1, 11900] loss: 0.54264\n",
            "[1, 12000] loss: 0.54438\n",
            "[1, 12100] loss: 0.53601\n",
            "[1, 12200] loss: 0.53909\n",
            "[1, 12300] loss: 0.54001\n",
            "[1, 12400] loss: 0.54159\n",
            "[1, 12500] loss: 0.53660\n",
            "[1,   100] loss: 0.46700\n",
            "[1,   200] loss: 0.45364\n",
            "[1,   300] loss: 0.45231\n",
            "[1,   400] loss: 0.44303\n",
            "[1,   500] loss: 0.45099\n",
            "[1,   600] loss: 0.44353\n",
            "[1,   700] loss: 0.44611\n",
            "[1,   800] loss: 0.44487\n",
            "[1,   900] loss: 0.44266\n",
            "[1,  1000] loss: 0.43570\n",
            "[1,  1100] loss: 0.43661\n",
            "[1,  1200] loss: 0.43835\n",
            "[1,  1300] loss: 0.43199\n",
            "[1,  1400] loss: 0.43343\n",
            "[1,  1500] loss: 0.43044\n",
            "[1,  1600] loss: 0.43156\n",
            "[1,  1700] loss: 0.43258\n",
            "[1,  1800] loss: 0.42836\n",
            "[1,  1900] loss: 0.43113\n",
            "[1,  2000] loss: 0.42803\n",
            "[1,  2100] loss: 0.42998\n",
            "[1,  2200] loss: 0.42382\n",
            "[1,  2300] loss: 0.42489\n",
            "[1,  2400] loss: 0.42386\n",
            "[1,  2500] loss: 0.42288\n",
            "[1,  2600] loss: 0.42240\n",
            "[1,  2700] loss: 0.42556\n",
            "[1,  2800] loss: 0.42359\n",
            "[1,  2900] loss: 0.42326\n",
            "[1,  3000] loss: 0.42312\n",
            "[1,  3100] loss: 0.42378\n",
            "[1,  3200] loss: 0.42200\n",
            "[1,  3300] loss: 0.42461\n",
            "[1,  3400] loss: 0.42461\n",
            "[1,  3500] loss: 0.42150\n",
            "[1,  3600] loss: 0.42172\n",
            "[1,  3700] loss: 0.42125\n",
            "[1,  3800] loss: 0.42121\n",
            "[1,  3900] loss: 0.42042\n",
            "[1,  4000] loss: 0.42018\n",
            "[1,  4100] loss: 0.41892\n",
            "[1,  4200] loss: 0.42337\n",
            "[1,  4300] loss: 0.42147\n",
            "[1,  4400] loss: 0.41855\n",
            "[1,  4500] loss: 0.41688\n",
            "[1,  4600] loss: 0.41807\n",
            "[1,  4700] loss: 0.41484\n",
            "[1,  4800] loss: 0.41982\n",
            "[1,  4900] loss: 0.41517\n",
            "[1,  5000] loss: 0.41789\n",
            "[1,  5100] loss: 0.41670\n",
            "[1,  5200] loss: 0.41735\n",
            "[1,  5300] loss: 0.41646\n",
            "[1,  5400] loss: 0.41908\n",
            "[1,  5500] loss: 0.41310\n",
            "[1,  5600] loss: 0.41676\n",
            "[1,  5700] loss: 0.41709\n",
            "[1,  5800] loss: 0.41124\n",
            "[1,  5900] loss: 0.41379\n",
            "[1,  6000] loss: 0.41722\n",
            "[1,  6100] loss: 0.41688\n",
            "[1,  6200] loss: 0.41547\n",
            "[1,  6300] loss: 0.41516\n",
            "[1,  6400] loss: 0.41366\n",
            "[1,  6500] loss: 0.41513\n",
            "[1,  6600] loss: 0.41602\n",
            "[1,  6700] loss: 0.41608\n",
            "[1,  6800] loss: 0.41316\n",
            "[1,  6900] loss: 0.41649\n",
            "[1,  7000] loss: 0.41601\n",
            "[1,  7100] loss: 0.41361\n",
            "[1,  7200] loss: 0.41361\n",
            "[1,  7300] loss: 0.41599\n",
            "[1,  7400] loss: 0.41747\n",
            "[1,  7500] loss: 0.41516\n",
            "[1,  7600] loss: 0.41134\n",
            "[1,  7700] loss: 0.41653\n",
            "[1,  7800] loss: 0.41513\n",
            "[1,  7900] loss: 0.41747\n",
            "[1,  8000] loss: 0.41850\n",
            "[1,  8100] loss: 0.41283\n",
            "[1,  8200] loss: 0.41297\n",
            "[1,  8300] loss: 0.41766\n",
            "[1,  8400] loss: 0.41872\n",
            "[1,  8500] loss: 0.41884\n",
            "[1,  8600] loss: 0.41641\n",
            "[1,  8700] loss: 0.41382\n",
            "[1,  8800] loss: 0.41253\n",
            "[1,  8900] loss: 0.41595\n",
            "[1,  9000] loss: 0.40895\n",
            "[1,  9100] loss: 0.41540\n",
            "[1,  9200] loss: 0.41237\n",
            "[1,  9300] loss: 0.41184\n",
            "[1,  9400] loss: 0.40935\n",
            "[1,  9500] loss: 0.41101\n",
            "[1,  9600] loss: 0.41804\n",
            "[1,  9700] loss: 0.41290\n",
            "[1,  9800] loss: 0.40896\n",
            "[1,  9900] loss: 0.41325\n",
            "[1, 10000] loss: 0.41095\n",
            "[1, 10100] loss: 0.41159\n",
            "[1, 10200] loss: 0.40848\n",
            "[1, 10300] loss: 0.41113\n",
            "[1, 10400] loss: 0.41033\n",
            "[1, 10500] loss: 0.40998\n",
            "[1, 10600] loss: 0.40962\n",
            "[1, 10700] loss: 0.41277\n",
            "[1, 10800] loss: 0.41184\n",
            "[1, 10900] loss: 0.40896\n",
            "[1, 11000] loss: 0.41044\n",
            "[1, 11100] loss: 0.41116\n",
            "[1, 11200] loss: 0.41586\n",
            "[1, 11300] loss: 0.40911\n",
            "[1, 11400] loss: 0.41559\n",
            "[1, 11500] loss: 0.40750\n",
            "[1, 11600] loss: 0.41034\n",
            "[1, 11700] loss: 0.40857\n",
            "[1, 11800] loss: 0.40620\n",
            "[1, 11900] loss: 0.41400\n",
            "[1, 12000] loss: 0.41062\n",
            "[1, 12100] loss: 0.40822\n",
            "[1, 12200] loss: 0.40960\n",
            "[1, 12300] loss: 0.40891\n",
            "[1, 12400] loss: 0.40921\n",
            "[1, 12500] loss: 0.40791\n",
            "[1,   100] loss: 0.38696\n",
            "[1,   200] loss: 0.38689\n",
            "[1,   300] loss: 0.38866\n",
            "[1,   400] loss: 0.38528\n",
            "[1,   500] loss: 0.38584\n",
            "[1,   600] loss: 0.38971\n",
            "[1,   700] loss: 0.38800\n",
            "[1,   800] loss: 0.39108\n",
            "[1,   900] loss: 0.39427\n",
            "[1,  1000] loss: 0.39304\n",
            "[1,  1100] loss: 0.39310\n",
            "[1,  1200] loss: 0.39319\n",
            "[1,  1300] loss: 0.39317\n",
            "[1,  1400] loss: 0.39682\n",
            "[1,  1500] loss: 0.39996\n",
            "[1,  1600] loss: 0.39762\n",
            "[1,  1700] loss: 0.39923\n",
            "[1,  1800] loss: 0.40255\n",
            "[1,  1900] loss: 0.39908\n",
            "[1,  2000] loss: 0.39676\n",
            "[1,  2100] loss: 0.39979\n",
            "[1,  2200] loss: 0.40160\n",
            "[1,  2300] loss: 0.39892\n",
            "[1,  2400] loss: 0.39999\n",
            "[1,  2500] loss: 0.40127\n",
            "[1,  2600] loss: 0.39778\n",
            "[1,  2700] loss: 0.39750\n",
            "[1,  2800] loss: 0.40061\n",
            "[1,  2900] loss: 0.40118\n",
            "[1,  3000] loss: 0.40389\n",
            "[1,  3100] loss: 0.40206\n",
            "[1,  3200] loss: 0.40060\n",
            "[1,  3300] loss: 0.40171\n",
            "[1,  3400] loss: 0.39935\n",
            "[1,  3500] loss: 0.40009\n",
            "[1,  3600] loss: 0.40021\n",
            "[1,  3700] loss: 0.40276\n",
            "[1,  3800] loss: 0.40111\n",
            "[1,  3900] loss: 0.40342\n",
            "[1,  4000] loss: 0.40277\n",
            "[1,  4100] loss: 0.40642\n",
            "[1,  4200] loss: 0.40028\n",
            "[1,  4300] loss: 0.40510\n",
            "[1,  4400] loss: 0.40205\n",
            "[1,  4500] loss: 0.40078\n",
            "[1,  4600] loss: 0.39960\n",
            "[1,  4700] loss: 0.40231\n",
            "[1,  4800] loss: 0.40565\n",
            "[1,  4900] loss: 0.40148\n",
            "[1,  5000] loss: 0.40396\n",
            "[1,  5100] loss: 0.40106\n",
            "[1,  5200] loss: 0.40116\n",
            "[1,  5300] loss: 0.39993\n",
            "[1,  5400] loss: 0.40339\n",
            "[1,  5500] loss: 0.40287\n",
            "[1,  5600] loss: 0.40187\n",
            "[1,  5700] loss: 0.40277\n",
            "[1,  5800] loss: 0.40072\n",
            "[1,  5900] loss: 0.40372\n",
            "[1,  6000] loss: 0.39977\n",
            "[1,  6100] loss: 0.39818\n",
            "[1,  6200] loss: 0.40378\n",
            "[1,  6300] loss: 0.40405\n",
            "[1,  6400] loss: 0.40223\n",
            "[1,  6500] loss: 0.40589\n",
            "[1,  6600] loss: 0.40038\n",
            "[1,  6700] loss: 0.40026\n",
            "[1,  6800] loss: 0.39848\n",
            "[1,  6900] loss: 0.40153\n",
            "[1,  7000] loss: 0.40066\n",
            "[1,  7100] loss: 0.40425\n",
            "[1,  7200] loss: 0.40418\n",
            "[1,  7300] loss: 0.40304\n",
            "[1,  7400] loss: 0.40253\n",
            "[1,  7500] loss: 0.40221\n",
            "[1,  7600] loss: 0.40102\n",
            "[1,  7700] loss: 0.40251\n",
            "[1,  7800] loss: 0.40001\n",
            "[1,  7900] loss: 0.40170\n",
            "[1,  8000] loss: 0.39973\n",
            "[1,  8100] loss: 0.40264\n",
            "[1,  8200] loss: 0.39972\n",
            "[1,  8300] loss: 0.40266\n",
            "[1,  8400] loss: 0.40563\n",
            "[1,  8500] loss: 0.40198\n",
            "[1,  8600] loss: 0.40028\n",
            "[1,  8700] loss: 0.40162\n",
            "[1,  8800] loss: 0.39888\n",
            "[1,  8900] loss: 0.39796\n",
            "[1,  9000] loss: 0.39946\n",
            "[1,  9100] loss: 0.40177\n",
            "[1,  9200] loss: 0.39930\n",
            "[1,  9300] loss: 0.40215\n",
            "[1,  9400] loss: 0.39776\n",
            "[1,  9500] loss: 0.40145\n",
            "[1,  9600] loss: 0.40334\n",
            "[1,  9700] loss: 0.39969\n",
            "[1,  9800] loss: 0.40395\n",
            "[1,  9900] loss: 0.40230\n",
            "[1, 10000] loss: 0.40228\n",
            "[1, 10100] loss: 0.40101\n",
            "[1, 10200] loss: 0.40091\n",
            "[1, 10300] loss: 0.39764\n",
            "[1, 10400] loss: 0.39918\n",
            "[1, 10500] loss: 0.39940\n",
            "[1, 10600] loss: 0.39915\n",
            "[1, 10700] loss: 0.39684\n",
            "[1, 10800] loss: 0.40091\n",
            "[1, 10900] loss: 0.39781\n",
            "[1, 11000] loss: 0.39656\n",
            "[1, 11100] loss: 0.39942\n",
            "[1, 11200] loss: 0.39993\n",
            "[1, 11300] loss: 0.40126\n",
            "[1, 11400] loss: 0.39863\n",
            "[1, 11500] loss: 0.39693\n",
            "[1, 11600] loss: 0.39826\n",
            "[1, 11700] loss: 0.40132\n",
            "[1, 11800] loss: 0.40063\n",
            "[1, 11900] loss: 0.39830\n",
            "[1, 12000] loss: 0.40047\n",
            "[1, 12100] loss: 0.39613\n",
            "[1, 12200] loss: 0.39729\n",
            "[1, 12300] loss: 0.40071\n",
            "[1, 12400] loss: 0.39996\n",
            "[1, 12500] loss: 0.40041\n",
            "[1,   100] loss: 0.35226\n",
            "[1,   200] loss: 0.34081\n",
            "[1,   300] loss: 0.33776\n",
            "[1,   400] loss: 0.33985\n",
            "[1,   500] loss: 0.33612\n",
            "[1,   600] loss: 0.33433\n",
            "[1,   700] loss: 0.33375\n",
            "[1,   800] loss: 0.33230\n",
            "[1,   900] loss: 0.33232\n",
            "[1,  1000] loss: 0.33015\n",
            "[1,  1100] loss: 0.33009\n",
            "[1,  1200] loss: 0.33138\n",
            "[1,  1300] loss: 0.32761\n",
            "[1,  1400] loss: 0.32890\n",
            "[1,  1500] loss: 0.32889\n",
            "[1,  1600] loss: 0.32922\n",
            "[1,  1700] loss: 0.32909\n",
            "[1,  1800] loss: 0.32973\n",
            "[1,  1900] loss: 0.32862\n",
            "[1,  2000] loss: 0.32614\n",
            "[1,  2100] loss: 0.33028\n",
            "[1,  2200] loss: 0.32476\n",
            "[1,  2300] loss: 0.32718\n",
            "[1,  2400] loss: 0.32626\n",
            "[1,  2500] loss: 0.32655\n",
            "[1,  2600] loss: 0.32566\n",
            "[1,  2700] loss: 0.32641\n",
            "[1,  2800] loss: 0.32639\n",
            "[1,  2900] loss: 0.32559\n",
            "[1,  3000] loss: 0.32416\n",
            "[1,  3100] loss: 0.32332\n",
            "[1,  3200] loss: 0.32639\n",
            "[1,  3300] loss: 0.32416\n",
            "[1,  3400] loss: 0.32448\n",
            "[1,  3500] loss: 0.32489\n",
            "[1,  3600] loss: 0.32466\n",
            "[1,  3700] loss: 0.32455\n",
            "[1,  3800] loss: 0.32623\n",
            "[1,  3900] loss: 0.32337\n",
            "[1,  4000] loss: 0.32625\n",
            "[1,  4100] loss: 0.32378\n",
            "[1,  4200] loss: 0.32351\n",
            "[1,  4300] loss: 0.32275\n",
            "[1,  4400] loss: 0.32624\n",
            "[1,  4500] loss: 0.32558\n",
            "[1,  4600] loss: 0.32498\n",
            "[1,  4700] loss: 0.31970\n",
            "[1,  4800] loss: 0.32387\n",
            "[1,  4900] loss: 0.32098\n",
            "[1,  5000] loss: 0.31874\n",
            "[1,  5100] loss: 0.32106\n",
            "[1,  5200] loss: 0.32316\n",
            "[1,  5300] loss: 0.32155\n",
            "[1,  5400] loss: 0.32660\n",
            "[1,  5500] loss: 0.32239\n",
            "[1,  5600] loss: 0.32100\n",
            "[1,  5700] loss: 0.32295\n",
            "[1,  5800] loss: 0.32286\n",
            "[1,  5900] loss: 0.32345\n",
            "[1,  6000] loss: 0.32291\n",
            "[1,  6100] loss: 0.32243\n",
            "[1,  6200] loss: 0.32122\n",
            "[1,  6300] loss: 0.32188\n",
            "[1,  6400] loss: 0.31798\n",
            "[1,  6500] loss: 0.31903\n",
            "[1,  6600] loss: 0.32284\n",
            "[1,  6700] loss: 0.31983\n",
            "[1,  6800] loss: 0.32058\n",
            "[1,  6900] loss: 0.32121\n",
            "[1,  7000] loss: 0.32227\n",
            "[1,  7100] loss: 0.32198\n",
            "[1,  7200] loss: 0.32333\n",
            "[1,  7300] loss: 0.32294\n",
            "[1,  7400] loss: 0.32027\n",
            "[1,  7500] loss: 0.31873\n",
            "[1,  7600] loss: 0.32198\n",
            "[1,  7700] loss: 0.32436\n",
            "[1,  7800] loss: 0.31895\n",
            "[1,  7900] loss: 0.32149\n",
            "[1,  8000] loss: 0.32133\n",
            "[1,  8100] loss: 0.31924\n",
            "[1,  8200] loss: 0.32061\n",
            "[1,  8300] loss: 0.31989\n",
            "[1,  8400] loss: 0.31914\n",
            "[1,  8500] loss: 0.32103\n",
            "[1,  8600] loss: 0.31896\n",
            "[1,  8700] loss: 0.32090\n",
            "[1,  8800] loss: 0.32052\n",
            "[1,  8900] loss: 0.31977\n",
            "[1,  9000] loss: 0.32256\n",
            "[1,  9100] loss: 0.32225\n",
            "[1,  9200] loss: 0.32095\n",
            "[1,  9300] loss: 0.32225\n",
            "[1,  9400] loss: 0.31855\n",
            "[1,  9500] loss: 0.31993\n",
            "[1,  9600] loss: 0.32039\n",
            "[1,  9700] loss: 0.31763\n",
            "[1,  9800] loss: 0.31987\n",
            "[1,  9900] loss: 0.32017\n",
            "[1, 10000] loss: 0.31920\n",
            "[1, 10100] loss: 0.31996\n",
            "[1, 10200] loss: 0.31936\n",
            "[1, 10300] loss: 0.31785\n",
            "[1, 10400] loss: 0.32039\n",
            "[1, 10500] loss: 0.31749\n",
            "[1, 10600] loss: 0.31988\n",
            "[1, 10700] loss: 0.31977\n",
            "[1, 10800] loss: 0.31833\n",
            "[1, 10900] loss: 0.31736\n",
            "[1, 11000] loss: 0.31736\n",
            "[1, 11100] loss: 0.31885\n",
            "[1, 11200] loss: 0.32008\n",
            "[1, 11300] loss: 0.31911\n",
            "[1, 11400] loss: 0.32197\n",
            "[1, 11500] loss: 0.31702\n",
            "[1, 11600] loss: 0.31959\n",
            "[1, 11700] loss: 0.31899\n",
            "[1, 11800] loss: 0.31951\n",
            "[1, 11900] loss: 0.31631\n",
            "[1, 12000] loss: 0.31991\n",
            "[1, 12100] loss: 0.31601\n",
            "[1, 12200] loss: 0.31682\n",
            "[1, 12300] loss: 0.31844\n",
            "[1, 12400] loss: 0.31687\n",
            "[1, 12500] loss: 0.31768\n",
            "[1,   100] loss: 0.31085\n",
            "[1,   200] loss: 0.31200\n",
            "[1,   300] loss: 0.31390\n",
            "[1,   400] loss: 0.31543\n",
            "[1,   500] loss: 0.31252\n",
            "[1,   600] loss: 0.31447\n",
            "[1,   700] loss: 0.31136\n",
            "[1,   800] loss: 0.31524\n",
            "[1,   900] loss: 0.31230\n",
            "[1,  1000] loss: 0.31060\n",
            "[1,  1100] loss: 0.31372\n",
            "[1,  1200] loss: 0.31395\n",
            "[1,  1300] loss: 0.31459\n",
            "[1,  1400] loss: 0.31332\n",
            "[1,  1500] loss: 0.31468\n",
            "[1,  1600] loss: 0.31815\n",
            "[1,  1700] loss: 0.31197\n",
            "[1,  1800] loss: 0.31368\n",
            "[1,  1900] loss: 0.31471\n",
            "[1,  2000] loss: 0.31307\n",
            "[1,  2100] loss: 0.31406\n",
            "[1,  2200] loss: 0.31440\n",
            "[1,  2300] loss: 0.31277\n",
            "[1,  2400] loss: 0.31199\n",
            "[1,  2500] loss: 0.31477\n",
            "[1,  2600] loss: 0.31198\n",
            "[1,  2700] loss: 0.31317\n",
            "[1,  2800] loss: 0.31422\n",
            "[1,  2900] loss: 0.31341\n",
            "[1,  3000] loss: 0.31478\n",
            "[1,  3100] loss: 0.31248\n",
            "[1,  3200] loss: 0.31371\n",
            "[1,  3300] loss: 0.31099\n",
            "[1,  3400] loss: 0.31348\n",
            "[1,  3500] loss: 0.31356\n",
            "[1,  3600] loss: 0.31040\n",
            "[1,  3700] loss: 0.31365\n",
            "[1,  3800] loss: 0.31356\n",
            "[1,  3900] loss: 0.31255\n",
            "[1,  4000] loss: 0.31639\n",
            "[1,  4100] loss: 0.31160\n",
            "[1,  4200] loss: 0.31265\n",
            "[1,  4300] loss: 0.31284\n",
            "[1,  4400] loss: 0.31181\n",
            "[1,  4500] loss: 0.30890\n",
            "[1,  4600] loss: 0.31198\n",
            "[1,  4700] loss: 0.31298\n",
            "[1,  4800] loss: 0.30965\n",
            "[1,  4900] loss: 0.31251\n",
            "[1,  5000] loss: 0.31210\n",
            "[1,  5100] loss: 0.31279\n",
            "[1,  5200] loss: 0.31233\n",
            "[1,  5300] loss: 0.31330\n",
            "[1,  5400] loss: 0.31168\n",
            "[1,  5500] loss: 0.31221\n",
            "[1,  5600] loss: 0.31402\n",
            "[1,  5700] loss: 0.31404\n",
            "[1,  5800] loss: 0.31361\n",
            "[1,  5900] loss: 0.31058\n",
            "[1,  6000] loss: 0.31231\n",
            "[1,  6100] loss: 0.31321\n",
            "[1,  6200] loss: 0.31081\n",
            "[1,  6300] loss: 0.31083\n",
            "[1,  6400] loss: 0.31229\n",
            "[1,  6500] loss: 0.31343\n",
            "[1,  6600] loss: 0.31147\n",
            "[1,  6700] loss: 0.31254\n",
            "[1,  6800] loss: 0.31302\n",
            "[1,  6900] loss: 0.30941\n",
            "[1,  7000] loss: 0.31112\n",
            "[1,  7100] loss: 0.30960\n",
            "[1,  7200] loss: 0.31390\n",
            "[1,  7300] loss: 0.31324\n",
            "[1,  7400] loss: 0.31220\n",
            "[1,  7500] loss: 0.30878\n",
            "[1,  7600] loss: 0.31267\n",
            "[1,  7700] loss: 0.31005\n",
            "[1,  7800] loss: 0.31135\n",
            "[1,  7900] loss: 0.31063\n",
            "[1,  8000] loss: 0.31262\n",
            "[1,  8100] loss: 0.31076\n",
            "[1,  8200] loss: 0.31404\n",
            "[1,  8300] loss: 0.31325\n",
            "[1,  8400] loss: 0.31121\n",
            "[1,  8500] loss: 0.30982\n",
            "[1,  8600] loss: 0.31003\n",
            "[1,  8700] loss: 0.31394\n",
            "[1,  8800] loss: 0.31240\n",
            "[1,  8900] loss: 0.31116\n",
            "[1,  9000] loss: 0.31024\n",
            "[1,  9100] loss: 0.30983\n",
            "[1,  9200] loss: 0.30840\n",
            "[1,  9300] loss: 0.31101\n",
            "[1,  9400] loss: 0.31359\n",
            "[1,  9500] loss: 0.31405\n",
            "[1,  9600] loss: 0.30811\n",
            "[1,  9700] loss: 0.31315\n",
            "[1,  9800] loss: 0.30989\n",
            "[1,  9900] loss: 0.30925\n",
            "[1, 10000] loss: 0.31315\n",
            "[1, 10100] loss: 0.31111\n",
            "[1, 10200] loss: 0.31071\n",
            "[1, 10300] loss: 0.30953\n",
            "[1, 10400] loss: 0.31317\n",
            "[1, 10500] loss: 0.31207\n",
            "[1, 10600] loss: 0.31066\n",
            "[1, 10700] loss: 0.30768\n",
            "[1, 10800] loss: 0.31211\n",
            "[1, 10900] loss: 0.31206\n",
            "[1, 11000] loss: 0.31271\n",
            "[1, 11100] loss: 0.31184\n",
            "[1, 11200] loss: 0.31024\n",
            "[1, 11300] loss: 0.31235\n",
            "[1, 11400] loss: 0.30971\n",
            "[1, 11500] loss: 0.31070\n",
            "[1, 11600] loss: 0.31152\n",
            "[1, 11700] loss: 0.30981\n",
            "[1, 11800] loss: 0.30989\n",
            "[1, 11900] loss: 0.30744\n",
            "[1, 12000] loss: 0.31266\n",
            "[1, 12100] loss: 0.31231\n",
            "[1, 12200] loss: 0.31209\n",
            "[1, 12300] loss: 0.31072\n",
            "[1, 12400] loss: 0.31135\n",
            "[1, 12500] loss: 0.30989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2bW2vKUaYyf",
        "outputId": "4dff23f0-e13c-45e3-9b78-61a90a690464"
      },
      "source": [
        "test_losses = test(model, test_loader_channel)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 75710.0/10000 (16%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "dw5Yn_p7ItE-",
        "outputId": "ee44b17e-c43f-4247-e38c-485b1d97bf06"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss)\n",
        "#plt.plot(test_losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7efd4daff690>]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbvQWh1jVcI"
      },
      "source": [
        "import time\n",
        "import torch.optim as optim\n",
        "\n",
        "def train(net, dataloader, optimizer, epoch=1, verbose=True):\n",
        "  net.to(device)\n",
        "  losses = []\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  sum_loss = 0.0\n",
        "  for i, batch in enumerate(dataloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = batch[0].to(device), batch[1].to(device)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize \n",
        "    outputs = net(inputs)\n",
        "    '''\n",
        "    y = torch.zeros([outputs.shape[0], 1, 9, 9])\n",
        "    for k in range(outputs.shape[0]):\n",
        "      for i in range(9):\n",
        "        for j in range(9):\n",
        "          y[k, 0, i, j] = torch.argmax(outputs[k, :, i, j])+1\n",
        "          print(y[k, 0, i, j])\n",
        "    '''\n",
        "    print(\"INPUT\")\n",
        "    #print(inputs)\n",
        "    #print(\"LABEL\")\n",
        "    #print(labels)\n",
        "    #print(\"OUTPUT\")\n",
        "    #print(outputs)\n",
        "    # break\n",
        "    \n",
        "    loss = criterion(outputs, torch.flatten(labels, 1))\n",
        "    print(loss)\n",
        "    break\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    losses.append(loss.item())\n",
        "    sum_loss += loss.item()\n",
        "    if i % 100 == 99:    # print every 100 mini-batches\n",
        "      if verbose:\n",
        "        print('[%d, %5d] loss: %.3f' %\n",
        "            (epoch, i + 1, sum_loss / 100))\n",
        "      sum_loss = 0.0\n",
        "    \n",
        "  return losses\n",
        "\n",
        "def test(model, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    blank_cnt = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (data, label) in enumerate(test_loader):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        output = model(data)\n",
        "        \n",
        "        blank_mask = torch.zeros([output.shape[0], 1, 9, 9]).floats()\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              is_blank = True\n",
        "              for n in range(9):\n",
        "                if data[k, n, i, j] != 0:\n",
        "                  is_blank = False\n",
        "                  break;\n",
        "              if is_blank:\n",
        "                blank_mask[k, 0, i, k] = 1.0\n",
        "                blank_cnt += 1\n",
        "        \n",
        "        x = torch.zeros([output.shape[0], 1, 9, 9])\n",
        "        for k in range(output.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              x[k, 0, i, j] = torch.argmax(output[k, :, i, j])+1\n",
        "        \n",
        "        y = torch.zeros([label.shape[0], 1, 9, 9])\n",
        "        for k in range(label.shape[0]):\n",
        "          for i in range(9):\n",
        "            for j in range(9):\n",
        "              y[k, 0, i, j] = torch.argmax(label[k, :, i, j])+1\n",
        "\n",
        "        correct_mask = x.eq(y.view_as(x))\n",
        "\n",
        "        num_correct = (correct_mask * blank_mask).sum().item()\n",
        "\n",
        "        correct += num_correct\n",
        "            \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_accuracy = 100. * correct / len(test_loader.dataset) /81\n",
        "    test_accuracy = 100. * correct / blank_cnt\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), test_accuracy))\n",
        "    return test_loss, test_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "Sm78W6f5jZUR",
        "outputId": "312b2206-5043-4ff1-9d94-5ca256865e6e"
      },
      "source": [
        "# Play around with these constants, you may find a better setting.\n",
        "BATCH_SIZE = 1\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0\n",
        "GAMMA = 0.9\n",
        "DATA_PATH = \"checkpoints/sudokuNetChannel\"\n",
        "\n",
        "EXPERIMENT_VERSION = \"0.1\" # increment this to start a new experiment\n",
        "\n",
        "# Now the actual training code\n",
        "use_cuda = USE_CUDA and torch.cuda.is_available()\n",
        "\n",
        "#torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "print('Using device', device)\n",
        "import multiprocessing\n",
        "print('num cpus:', multiprocessing.cpu_count())\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Simple sudokuNet\n",
        "Parameters:\n",
        "BATCH_SIZE = 32\n",
        "TEST_BATCH_SIZE = 10\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.5\n",
        "USE_CUDA = True\n",
        "SEED = 0\n",
        "PRINT_INTERVAL = 100\n",
        "WEIGHT_DECAY = 0\n",
        "GAMMA = 0.9\n",
        "DATA_PATH = \"checkpoints/sudokuNet\"\n",
        "\"\"\"\n",
        "# model = sudokuNet().to(device)\n",
        "# train_loader = DataLoader(sudokuDataset(train_data), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "# test_loader = DataLoader(sudokuDataset(test_data), batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "\"\"\"\n",
        "9-Channel sudokuNet\n",
        "\"\"\"\n",
        "model = sudokuNetChannel().to(device)\n",
        "train_loader = DataLoader(sudokuDatasetChannel(train_data), batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(sudokuDatasetChannel(test_data), batch_size=TEST_BATCH_SIZE, shuffle=True)\n",
        "\n",
        "start_epoch = 1\n",
        "if os.listdir(DATA_PATH):\n",
        " start_epoch = model.load_last_model(DATA_PATH + 'checkpoints')\n",
        " print(f\"loaded from {DATA_PATH}, starting from epoch {start_epoch}\")\n",
        "\n",
        "lr = LEARNING_RATE * (np.power(GAMMA, start_epoch))\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY, momentum=MOMENTUM)\n",
        "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, GAMMA)\n",
        "\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS+1):\n",
        "  print(f\"CURRENT EPOCH: {epoch}\")\n",
        "  train_loss = train(model, train_loader, optimizer, epoch-1)\n",
        "  _, accuracy = test(model, test_loader)\n",
        "  losses.extend(train_loss)\n",
        "  # accuracies.append(accuracy)\n",
        "  model.save_best_model(accuracy, DATA_PATH+f\"/epoch{epoch}.pt\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cpu\n",
            "num cpus: 2\n",
            "CURRENT EPOCH: 1\n",
            "INPUT\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c84c5fd2fa5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CURRENT EPOCH: {epoch}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-a6e24ef1ed55>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, dataloader, optimizer, epoch, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: only batches of spatial targets supported (3D tensors) but got targets of dimension: 2"
          ]
        }
      ]
    }
  ]
}